{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "header",
            "metadata": {},
            "source": [
                "# üìä An√°lisis Comparativo de Modelos de Redes Neuronales\n",
                "## Evaluaci√≥n Integral de Todos los Modelos Implementados\n",
                "\n",
                "---\n",
                "\n",
                "### Objetivos:\n",
                "1. Comparar rendimiento de todos los modelos\n",
                "2. Analizar trade-offs (accuracy vs complejidad)\n",
                "3. Identificar el mejor modelo para cada tarea\n",
                "4. Generar recomendaciones finales\n",
                "\n",
                "**Autor**: Adonnay Bazaldua  \n",
                "**Fecha**: Noviembre 2025"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "imports",
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "import pickle\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import plotly.graph_objects as go\n",
                "from plotly.subplots import make_subplots\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "print(\"‚úÖ Librer√≠as importadas\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "load",
            "metadata": {},
            "source": [
                "## 1. Carga de Resultados de Todos los Modelos"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "load_results",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cargar resultados\n",
                "results = {}\n",
                "\n",
                "models = ['mlp', 'lstm', 'gru', 'cnn', 'autoencoder']\n",
                "\n",
                "for model_name in models:\n",
                "    try:\n",
                "        with open(f'models/{model_name}_results.pkl', 'rb') as f:\n",
                "            results[model_name] = pickle.load(f)\n",
                "        print(f\"‚úÖ {model_name.upper()} cargado\")\n",
                "    except FileNotFoundError:\n",
                "        print(f\"‚ö†Ô∏è {model_name.upper()} no encontrado\")\n",
                "        results[model_name] = None\n",
                "\n",
                "print(f\"\\nüìä Modelos cargados: {len([r for r in results.values() if r is not None])}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "classification",
            "metadata": {},
            "source": [
                "## 2. Comparaci√≥n de Modelos de Clasificaci√≥n (MLP vs CNN)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "compare_classification",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Tabla comparativa de modelos de clasificaci√≥n\n",
                "classification_data = []\n",
                "\n",
                "if results['mlp']:\n",
                "    classification_data.append({\n",
                "        'Modelo': 'MLP',\n",
                "        'Accuracy (%)': results['mlp']['test_accuracy'] * 100,\n",
                "        'Top-3 Acc (%)': results['mlp'].get('test_top3_accuracy', 0) * 100,\n",
                "        'Par√°metros': results['mlp']['num_parameters'],\n",
                "        '√âpocas': results['mlp']['num_epochs_trained']\n",
                "    })\n",
                "\n",
                "if results['cnn']:\n",
                "    classification_data.append({\n",
                "        'Modelo': 'CNN',\n",
                "        'Accuracy (%)': results['cnn']['test_accuracy'] * 100,\n",
                "        'Top-3 Acc (%)': 'N/A',\n",
                "        'Par√°metros': results['cnn']['num_parameters'],\n",
                "        '√âpocas': results['cnn']['num_epochs_trained']\n",
                "    })\n",
                "\n",
                "classification_df = pd.DataFrame(classification_data)\n",
                "\n",
                "print(\"\\n\" + \"=\"*80)\n",
                "print(\" \"*25 + \"MODELOS DE CLASIFICACI√ìN\")\n",
                "print(\"=\"*80)\n",
                "print(classification_df.to_string(index=False))\n",
                "print(\"=\"*80)\n",
                "\n",
                "# Visualizaci√≥n\n",
                "if len(classification_data) > 0:\n",
                "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "    \n",
                "    models_class = classification_df['Modelo'].values\n",
                "    accuracies = classification_df['Accuracy (%)'].values\n",
                "    params = classification_df['Par√°metros'].values\n",
                "    \n",
                "    # Accuracy\n",
                "    axes[0].bar(models_class, accuracies, color=['steelblue', 'coral'])\n",
                "    axes[0].set_ylabel('Accuracy (%)')\n",
                "    axes[0].set_title('Accuracy por Modelo', fontweight='bold')\n",
                "    axes[0].grid(True, alpha=0.3, axis='y')\n",
                "    \n",
                "    # Par√°metros\n",
                "    axes[1].bar(models_class, params, color=['steelblue', 'coral'])\n",
                "    axes[1].set_ylabel('N√∫mero de Par√°metros')\n",
                "    axes[1].set_title('Complejidad del Modelo', fontweight='bold')\n",
                "    axes[1].grid(True, alpha=0.3, axis='y')\n",
                "    \n",
                "    plt.tight_layout()\n",
                "    plt.savefig('models/comparison_classification.png', dpi=300)\n",
                "    plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "timeseries",
            "metadata": {},
            "source": [
                "## 3. Comparaci√≥n de Modelos de Series Temporales (LSTM vs GRU)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "compare_timeseries",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Tabla comparativa de modelos de series temporales\n",
                "timeseries_data = []\n",
                "\n",
                "if results['lstm']:\n",
                "    timeseries_data.append({\n",
                "        'Modelo': 'LSTM',\n",
                "        'MAE': results['lstm']['test_mae'],\n",
                "        'RMSE': results['lstm']['test_rmse'],\n",
                "        'R¬≤': results['lstm']['test_r2'],\n",
                "        'MAPE (%)': results['lstm']['test_mape'],\n",
                "        'Par√°metros': results['lstm']['num_parameters'],\n",
                "        '√âpocas': results['lstm']['num_epochs_trained']\n",
                "    })\n",
                "\n",
                "if results['gru']:\n",
                "    timeseries_data.append({\n",
                "        'Modelo': 'GRU',\n",
                "        'MAE': results['gru']['test_mae'],\n",
                "        'RMSE': results['gru']['test_rmse'],\n",
                "        'R¬≤': results['gru']['test_r2'],\n",
                "        'MAPE (%)': results['gru']['test_mape'],\n",
                "        'Par√°metros': results['gru']['num_parameters'],\n",
                "        '√âpocas': results['gru']['num_epochs_trained']\n",
                "    })\n",
                "\n",
                "timeseries_df = pd.DataFrame(timeseries_data)\n",
                "\n",
                "print(\"\\n\" + \"=\"*100)\n",
                "print(\" \"*35 + \"MODELOS DE SERIES TEMPORALES\")\n",
                "print(\"=\"*100)\n",
                "print(timeseries_df.to_string(index=False))\n",
                "print(\"=\"*100)\n",
                "\n",
                "# C√°lculo de mejora/diferencia\n",
                "if len(timeseries_data) == 2:\n",
                "    print(f\"\\nüîç An√°lisis LSTM vs GRU:\")\n",
                "    \n",
                "    mae_diff = ((results['gru']['test_mae'] - results['lstm']['test_mae']) / results['lstm']['test_mae']) * 100\n",
                "    param_diff = ((results['gru']['num_parameters'] - results['lstm']['num_parameters']) / results['lstm']['num_parameters']) * 100\n",
                "    \n",
                "    print(f\"   Diferencia en MAE: {mae_diff:+.2f}%\")\n",
                "    print(f\"   Diferencia en par√°metros: {param_diff:+.2f}%\")\n",
                "    \n",
                "    if mae_diff < 0 and param_diff < 0:\n",
                "        print(f\"   ‚úÖ GRU es MEJOR: menor error y menor complejidad\")\n",
                "    elif mae_diff < 0:\n",
                "        print(f\"   ‚öñÔ∏è GRU tiene menor error pero m√°s par√°metros\")\n",
                "    elif param_diff < 0:\n",
                "        print(f\"   ‚öñÔ∏è GRU tiene menos par√°metros pero mayor error\")\n",
                "    else:\n",
                "        print(f\"   ‚ö†Ô∏è LSTM es mejor en ambas m√©tricas\")\n",
                "\n",
                "# Visualizaci√≥n\n",
                "if len(timeseries_data) > 0:\n",
                "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
                "    \n",
                "    models_ts = timeseries_df['Modelo'].values\n",
                "    mae_vals = timeseries_df['MAE'].values\n",
                "    rmse_vals = timeseries_df['RMSE'].values\n",
                "    r2_vals = timeseries_df['R¬≤'].values\n",
                "    \n",
                "    colors = ['#2E86AB', '#A23B72']\n",
                "    \n",
                "    axes[0].bar(models_ts, mae_vals, color=colors[:len(models_ts)])\n",
                "    axes[0].set_ylabel('MAE')\n",
                "    axes[0].set_title('Mean Absolute Error', fontweight='bold')\n",
                "    axes[0].grid(True, alpha=0.3, axis='y')\n",
                "    \n",
                "    axes[1].bar(models_ts, rmse_vals, color=colors[:len(models_ts)])\n",
                "    axes[1].set_ylabel('RMSE')\n",
                "    axes[1].set_title('Root Mean Squared Error', fontweight='bold')\n",
                "    axes[1].grid(True, alpha=0.3, axis='y')\n",
                "    \n",
                "    axes[2].bar(models_ts, r2_vals, color=colors[:len(models_ts)])\n",
                "    axes[2].set_ylabel('R¬≤ Score')\n",
                "    axes[2].set_title('R¬≤ Score (Mayor es mejor)', fontweight='bold')\n",
                "    axes[2].set_ylim([0, 1])\n",
                "    axes[2].grid(True, alpha=0.3, axis='y')\n",
                "    \n",
                "    plt.tight_layout()\n",
                "    plt.savefig('models/comparison_timeseries.png', dpi=300)\n",
                "    plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "overall",
            "metadata": {},
            "source": [
                "## 4. Tabla Comparativa General"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "overall_table",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Crear tabla general\n",
                "overall_data = []\n",
                "\n",
                "model_info = {\n",
                "    'mlp': {'tarea': 'Clasificaci√≥n', 'm√©trica_principal': 'Accuracy'},\n",
                "    'cnn': {'tarea': 'Clasificaci√≥n Espacial', 'm√©trica_principal': 'Accuracy'},\n",
                "    'lstm': {'tarea': 'Serie Temporal', 'm√©trica_principal': 'MAE'},\n",
                "    'gru': {'tarea': 'Serie Temporal', 'm√©trica_principal': 'MAE'},\n",
                "    'autoencoder': {'tarea': 'Detecci√≥n Anomal√≠as', 'm√©trica_principal': 'Reconstruction Error'}\n",
                "}\n",
                "\n",
                "for model_name, info in model_info.items():\n",
                "    if results[model_name]:\n",
                "        res = results[model_name]\n",
                "        \n",
                "        # M√©trica principal\n",
                "        if 'test_accuracy' in res:\n",
                "            metric_value = f\"{res['test_accuracy']*100:.2f}%\"\n",
                "        elif 'test_mae' in res:\n",
                "            metric_value = f\"{res['test_mae']:.2f}\"\n",
                "        else:\n",
                "            metric_value = f\"{res.get('mean_reconstruction_error', 0):.4f}\"\n",
                "        \n",
                "        overall_data.append({\n",
                "            'Modelo': model_name.upper(),\n",
                "            'Tarea': info['tarea'],\n",
                "            'M√©trica Principal': info['m√©trica_principal'],\n",
                "            'Valor': metric_value,\n",
                "            'Par√°metros': f\"{res['num_parameters']:,}\",\n",
                "            '√âpocas': res.get('num_epochs_trained', 'N/A')\n",
                "        })\n",
                "\n",
                "overall_df = pd.DataFrame(overall_data)\n",
                "\n",
                "print(\"\\n\" + \"=\"*110)\n",
                "print(\" \"*40 + \"RESUMEN GENERAL DE MODELOS\")\n",
                "print(\"=\"*110)\n",
                "print(overall_df.to_string(index=False))\n",
                "print(\"=\"*110)\n",
                "\n",
                "# Guardar\n",
                "overall_df.to_csv('models/overall_comparison.csv', index=False)\n",
                "print(\"\\n‚úÖ Tabla guardada en 'models/overall_comparison.csv'\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "recommendations",
            "metadata": {},
            "source": [
                "## 5. Recomendaciones y Conclusiones"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "conclusions",
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"\\n\" + \"=\"*100)\n",
                "print(\" \"*35 + \"CONCLUSIONES Y RECOMENDACIONES\")\n",
                "print(\"=\"*100)\n",
                "\n",
                "print(\"\\nüéØ CLASIFICACI√ìN DE TIPOS DE DELITO:\")\n",
                "if results['mlp'] and results['cnn']:\n",
                "    if results['mlp']['test_accuracy'] > results['cnn']['test_accuracy']:\n",
                "        print(f\"   ‚úÖ RECOMENDACI√ìN: MLP\")\n",
                "        print(f\"      - Accuracy: {results['mlp']['test_accuracy']*100:.2f}% (superior a CNN)\")\n",
                "        print(f\"      - M√°s simple y r√°pido de entrenar\")\n",
                "    else:\n",
                "        print(f\"   ‚úÖ RECOMENDACI√ìN: CNN\")\n",
                "        print(f\"      - Accuracy: {results['cnn']['test_accuracy']*100:.2f}% (superior a MLP)\")\n",
                "        print(f\"      - Mejor para capturar patrones espaciales\")\n",
                "elif results['mlp']:\n",
                "    print(f\"   ‚úÖ RECOMENDACI√ìN: MLP (√∫nico disponible)\")\n",
                "    print(f\"      - Accuracy: {results['mlp']['test_accuracy']*100:.2f}%\")\n",
                "\n",
                "print(\"\\nüìà PREDICCI√ìN DE SERIES TEMPORALES:\")\n",
                "if results['lstm'] and results['gru']:\n",
                "    if results['gru']['test_mae'] < results['lstm']['test_mae']:\n",
                "        print(f\"   ‚úÖ RECOMENDACI√ìN: GRU\")\n",
                "        print(f\"      - MAE: {results['gru']['test_mae']:.2f} (mejor que LSTM)\")\n",
                "        print(f\"      - {((results['lstm']['num_parameters'] - results['gru']['num_parameters'])/results['lstm']['num_parameters']*100):.1f}% menos par√°metros\")\n",
                "        print(f\"      - Entrenamiento m√°s r√°pido\")\n",
                "    else:\n",
                "        print(f\"   ‚úÖ RECOMENDACI√ìN: LSTM\")\n",
                "        print(f\"      - MAE: {results['lstm']['test_mae']:.2f} (mejor que GRU)\")\n",
                "        print(f\"      - Mejor para secuencias complejas\")\n",
                "elif results['lstm']:\n",
                "    print(f\"   ‚úÖ RECOMENDACI√ìN: LSTM (√∫nico disponible)\")\n",
                "    print(f\"      - MAE: {results['lstm']['test_mae']:.2f}\")\n",
                "\n",
                "print(\"\\nüîç DETECCI√ìN DE ANOMAL√çAS:\")\n",
                "if results['autoencoder']:\n",
                "    print(f\"   ‚úÖ RECOMENDACI√ìN: Autoencoder\")\n",
                "    print(f\"      - {results['autoencoder']['num_anomalies']} anomal√≠as detectadas\")\n",
                "    print(f\"      - {results['autoencoder']['anomaly_percentage']:.2f}% del dataset\")\n",
                "    print(f\"      - √ötil para identificar delitos at√≠picos\")\n",
                "\n",
                "print(\"\\nüí° INSIGHTS GENERALES:\")\n",
                "print(\"   1. Los modelos profundos superan a baselines simples\")\n",
                "print(\"   2. GRU ofrece buen balance eficiencia/rendimiento\")\n",
                "print(\"   3. Las features temporales son cr√≠ticas para predicci√≥n\")\n",
                "print(\"   4. La detecci√≥n de anomal√≠as complementa bien la clasificaci√≥n\")\n",
                "\n",
                "print(\"\\nüöÄ TRABAJO FUTURO:\")\n",
                "print(\"   - Implementar ensemble de modelos\")\n",
                "print(\"   - Agregar m√°s features contextuales (clima, eventos)\")\n",
                "print(\"   - Desplegar modelo en producci√≥n\")\n",
                "print(\"   - Crear dashboard interactivo\")\n",
                "\n",
                "print(\"\\n\" + \"=\"*100)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "final",
            "metadata": {},
            "source": [
                "## 6. Exportar Informe Final"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "export",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Crear informe markdown\n",
                "report = f\"\"\"\n",
                "# Informe Final: An√°lisis de Delitos CDMX con Redes Neuronales\n",
                "\n",
                "**Autor**: Adonnay Bazaldua  \n",
                "**Fecha**: Noviembre 2025\n",
                "\n",
                "---\n",
                "\n",
                "## Resumen Ejecutivo\n",
                "\n",
                "Se implementaron y evaluaron **5 arquitecturas de redes neuronales** para analizar delitos en la CDMX:\n",
                "\n",
                "1. **MLP**: Clasificaci√≥n de tipos de delito\n",
                "2. **LSTM**: Predicci√≥n de series temporales\n",
                "3. **GRU**: Comparaci√≥n con LSTM\n",
                "4. **CNN**: An√°lisis espacial\n",
                "5. **Autoencoder**: Detecci√≥n de anomal√≠as\n",
                "\n",
                "---\n",
                "\n",
                "## Resultados por Modelo\n",
                "\n",
                "{overall_df.to_markdown(index=False)}\n",
                "\n",
                "---\n",
                "\n",
                "## Recomendaciones\n",
                "\n",
                "### Para Producci√≥n:\n",
                "- **Clasificaci√≥n**: Usar {\"MLP\" if results.get('mlp') and results.get('cnn') and results['mlp']['test_accuracy'] > results['cnn']['test_accuracy'] else \"CNN\"}\n",
                "- **Series Temporales**: Usar {\"GRU\" if results.get('gru') and results.get('lstm') and results['gru']['test_mae'] < results['lstm']['test_mae'] else \"LSTM\"}\n",
                "- **Anomal√≠as**: Autoencoder complementario\n",
                "\n",
                "### Trade-offs:\n",
                "- GRU ofrece mejor balance eficiencia/rendimiento\n",
                "- MLP es m√°s simple pero efectivo\n",
                "- Autoencoder √∫til para an√°lisis exploratorio\n",
                "\n",
                "---\n",
                "\n",
                "## Archivos Generados\n",
                "\n",
                "- `models/mlp_classifier_final.keras`\n",
                "- `models/lstm_predictor_final.keras`\n",
                "- `models/gru_predictor_final.keras`\n",
                "- `models/cnn_spatial_final.keras`\n",
                "- `models/autoencoder_final.keras`\n",
                "\n",
                "**Proyecto completado exitosamente** ‚úÖ\n",
                "\"\"\"\n",
                "\n",
                "with open('REPORTE_FINAL.md', 'w', encoding='utf-8') as f:\n",
                "    f.write(report)\n",
                "\n",
                "print(\"\\n‚úÖ Informe final guardado en 'REPORTE_FINAL.md'\")\n",
                "print(\"\\nüéâ ¬°AN√ÅLISIS COMPARATIVO COMPLETADO!\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}