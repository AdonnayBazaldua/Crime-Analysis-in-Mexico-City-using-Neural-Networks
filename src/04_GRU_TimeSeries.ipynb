{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "header",
            "metadata": {},
            "source": [
                "# üîÑ GRU - Predicci√≥n de Series Temporales y Comparaci√≥n con LSTM\n",
                "## Gated Recurrent Unit: Alternativa Eficiente a LSTM\n",
                "\n",
                "---\n",
                "\n",
                "### Objetivos:\n",
                "1. Implementar modelo GRU con arquitectura similar a LSTM\n",
                "2. Entrenar y evaluar el modelo\n",
                "3. Comparar rendimiento con LSTM (accuracy, tiempo, par√°metros)\n",
                "4. Determinar cu√°l modelo es m√°s eficiente para este problema\n",
                "\n",
                "**Autor**: Adonnay Bazaldua  \n",
                "**Fecha**: Noviembre 2025"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "theory",
            "metadata": {},
            "source": [
                "## Teor√≠a: GRU vs LSTM\n",
                "\n",
                "### LSTM (Long Short-Term Memory)\n",
                "- **Compuertas**: 3 (Input, Forget, Output)\n",
                "- **Estados**: Cell state + Hidden state\n",
                "- **Par√°metros**: M√°s pesado\n",
                "- **Ventaja**: Mejor para secuencias muy largas\n",
                "\n",
                "### GRU (Gated Recurrent Unit)\n",
                "- **Compuertas**: 2 (Reset, Update)\n",
                "- **Estados**: Solo Hidden state\n",
                "- **Par√°metros**: ~25% menos que LSTM\n",
                "- **Ventaja**: M√°s r√°pido, menos propenso a overfitting\n",
                "\n",
                "**Pregunta clave**: ¬øEl trade-off de eficiencia vale la pena para nuestro caso?"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "imports",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Importaciones (igual que LSTM)\n",
                "import tensorflow as tf\n",
                "from tensorflow import keras\n",
                "from tensorflow.keras import layers, models, callbacks\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import pickle\n",
                "from sklearn.preprocessing import MinMaxScaler\n",
                "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import time\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "np.random.seed(42)\n",
                "tf.random.set_seed(42)\n",
                "\n",
                "print(f\"‚úÖ TensorFlow version: {tf.__version__}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "load_data",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cargar datos preprocesados (mismos que LSTM para comparaci√≥n justa)\n",
                "print(\"üìÇ Cargando datos de series temporales...\\n\")\n",
                "\n",
                "df_timeseries = pd.read_csv('processed_data/timeseries_data.csv')\n",
                "df_timeseries['fecha'] = pd.to_datetime(df_timeseries['fecha'])\n",
                "df_timeseries = df_timeseries.sort_values('fecha').reset_index(drop=True)\n",
                "\n",
                "print(f\"‚úÖ Datos cargados: {len(df_timeseries)} d√≠as\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "prepare_sequences",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Preparar secuencias (funci√≥n reutilizable)\n",
                "def create_sequences(data, seq_length):\n",
                "    X, y = [], []\n",
                "    for i in range(len(data) - seq_length):\n",
                "        X.append(data[i:i + seq_length])\n",
                "        y.append(data[i + seq_length])\n",
                "    return np.array(X), np.array(y)\n",
                "\n",
                "# Preparar datos\n",
                "data = df_timeseries['total_delitos'].values.reshape(-1, 1)\n",
                "scaler_gru = MinMaxScaler(feature_range=(0, 1))\n",
                "data_scaled = scaler_gru.fit_transform(data)\n",
                "\n",
                "SEQ_LENGTH = 30\n",
                "X_seq, y_seq = create_sequences(data_scaled, SEQ_LENGTH)\n",
                "\n",
                "# Divisi√≥n 70/15/15\n",
                "train_size = int(0.70 * len(X_seq))\n",
                "val_size = int(0.15 * len(X_seq))\n",
                "\n",
                "X_train_gru = X_seq[:train_size]\n",
                "y_train_gru = y_seq[:train_size]\n",
                "X_val_gru = X_seq[train_size:train_size + val_size]\n",
                "y_val_gru = y_seq[train_size:train_size + val_size]\n",
                "X_test_gru = X_seq[train_size + val_size:]\n",
                "y_test_gru = y_seq[train_size + val_size:]\n",
                "\n",
                "print(f\"‚úÖ Secuencias creadas: {X_seq.shape}\")\n",
                "print(f\"   Train: {X_train_gru.shape[0]} | Val: {X_val_gru.shape[0]} | Test: {X_test_gru.shape[0]}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "build_model",
            "metadata": {},
            "source": [
                "## Construcci√≥n del Modelo GRU\n",
                "\n",
                "### Arquitectura (paralela a LSTM):\n",
                "```\n",
                "Input(30, 1)\n",
                "  ‚Üí GRU(128, return_sequences=True) ‚Üí Dropout(0.2)\n",
                "  ‚Üí GRU(64) ‚Üí Dropout(0.2)\n",
                "  ‚Üí Dense(32, activation='relu')\n",
                "  ‚Üí Dense(1, activation='linear')\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "create_gru",
            "metadata": {},
            "outputs": [],
            "source": [
                "def create_gru_model(seq_length, n_features, learning_rate=0.001):\n",
                "    model = models.Sequential([\n",
                "        layers.Input(shape=(seq_length, n_features)),\n",
                "        \n",
                "        # GRU layers (en lugar de LSTM)\n",
                "        layers.GRU(128, return_sequences=True, activation='tanh'),\n",
                "        layers.Dropout(0.2),\n",
                "        \n",
                "        layers.GRU(64, activation='tanh'),\n",
                "        layers.Dropout(0.2),\n",
                "        \n",
                "        layers.Dense(32, activation='relu'),\n",
                "        layers.Dense(1, activation='linear')\n",
                "    ], name='GRU_Crime_Predictor')\n",
                "    \n",
                "    model.compile(\n",
                "        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
                "        loss='mse',\n",
                "        metrics=['mae', 'mse']\n",
                "    )\n",
                "    \n",
                "    return model\n",
                "\n",
                "# Crear modelo\n",
                "print(\"üèóÔ∏è Construyendo modelo GRU...\\n\")\n",
                "gru_model = create_gru_model(seq_length=SEQ_LENGTH, n_features=1)\n",
                "\n",
                "gru_model.summary()\n",
                "\n",
                "total_params_gru = gru_model.count_params()\n",
                "print(f\"\\nüìä Total de par√°metros GRU: {total_params_gru:,}\")\n",
                "\n",
                "# Cargar LSTM para comparaci√≥n\n",
                "try:\n",
                "    with open('models/lstm_results.pkl', 'rb') as f:\n",
                "        lstm_results = pickle.load(f)\n",
                "    lstm_params = lstm_results['num_parameters']\n",
                "    print(f\"üìä Total de par√°metros LSTM: {lstm_params:,}\")\n",
                "    print(f\"   Reducci√≥n: {(1 - total_params_gru/lstm_params)*100:.1f}% menos par√°metros\")\n",
                "except:\n",
                "    print(\"‚ö†Ô∏è No se encontr√≥ modelo LSTM para comparar\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "train",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Callbacks\n",
                "callbacks_list = [\n",
                "    callbacks.EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True, verbose=1),\n",
                "    callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=7, min_lr=1e-7, verbose=1),\n",
                "    callbacks.ModelCheckpoint('models/gru_best.keras', monitor='val_mae', save_best_only=True, verbose=1)\n",
                "]\n",
                "\n",
                "# Entrenar y medir tiempo\n",
                "print(\"üöÄ Iniciando entrenamiento GRU...\\n\")\n",
                "start_time = time.time()\n",
                "\n",
                "history_gru = gru_model.fit(\n",
                "    X_train_gru, y_train_gru,\n",
                "    batch_size=32,\n",
                "    epochs=100,\n",
                "    validation_data=(X_val_gru, y_val_gru),\n",
                "    callbacks=callbacks_list,\n",
                "    verbose=1\n",
                ")\n",
                "\n",
                "training_time_gru = time.time() - start_time\n",
                "print(f\"\\n‚è±Ô∏è Tiempo de entrenamiento GRU: {training_time_gru:.2f} segundos ({training_time_gru/60:.2f} min)\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "evaluate",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Evaluar\n",
                "print(\"üìä Evaluando modelo GRU...\\n\")\n",
                "\n",
                "y_pred_gru = gru_model.predict(X_test_gru, verbose=0)\n",
                "\n",
                "# Desnormalizar\n",
                "y_pred_gru_inv = scaler_gru.inverse_transform(y_pred_gru)\n",
                "y_test_gru_inv = scaler_gru.inverse_transform(y_test_gru.reshape(-1, 1))\n",
                "\n",
                "# M√©tricas\n",
                "def calculate_metrics(y_true, y_pred):\n",
                "    mae = mean_absolute_error(y_true, y_pred)\n",
                "    mse = mean_squared_error(y_true, y_pred)\n",
                "    rmse = np.sqrt(mse)\n",
                "    r2 = r2_score(y_true, y_pred)\n",
                "    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
                "    return mae, rmse, r2, mape\n",
                "\n",
                "gru_metrics = calculate_metrics(y_test_gru_inv, y_pred_gru_inv)\n",
                "\n",
                "print(\"üéØ Resultados GRU:\")\n",
                "print(f\"   MAE: {gru_metrics[0]:.2f}\")\n",
                "print(f\"   RMSE: {gru_metrics[1]:.2f}\")\n",
                "print(f\"   R¬≤: {gru_metrics[2]:.4f}\")\n",
                "print(f\"   MAPE: {gru_metrics[3]:.2f}%\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "comparison",
            "metadata": {},
            "source": [
                "## Comparaci√≥n Directa: GRU vs LSTM"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "compare",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Tabla comparativa\n",
                "try:\n",
                "    comparison_df = pd.DataFrame({\n",
                "        'M√©trica': ['MAE', 'RMSE', 'R¬≤', 'MAPE (%)', 'Par√°metros', 'Tiempo (min)', '√âpocas'],\n",
                "        'LSTM': [\n",
                "            lstm_results['test_mae'],\n",
                "            lstm_results['test_rmse'],\n",
                "            lstm_results['test_r2'],\n",
                "            lstm_results['test_mape'],\n",
                "            lstm_params,\n",
                "            'N/A',  # No guardamos tiempo de LSTM\n",
                "            lstm_results['num_epochs_trained']\n",
                "        ],\n",
                "        'GRU': [\n",
                "            gru_metrics[0],\n",
                "            gru_metrics[1],\n",
                "            gru_metrics[2],\n",
                "            gru_metrics[3],\n",
                "            total_params_gru,\n",
                "            f\"{training_time_gru/60:.2f}\",\n",
                "            len(history_gru.history['loss'])\n",
                "        ],\n",
                "        'Diferencia': [\n",
                "            f\"{((gru_metrics[0] - lstm_results['test_mae'])/lstm_results['test_mae']*100):+.1f}%\",\n",
                "            f\"{((gru_metrics[1] - lstm_results['test_rmse'])/lstm_results['test_rmse']*100):+.1f}%\",\n",
                "            f\"{((gru_metrics[2] - lstm_results['test_r2'])/lstm_results['test_r2']*100):+.1f}%\",\n",
                "            f\"{((gru_metrics[3] - lstm_results['test_mape'])/lstm_results['test_mape']*100):+.1f}%\",\n",
                "            f\"{((total_params_gru - lstm_params)/lstm_params*100):+.1f}%\",\n",
                "            'N/A',\n",
                "            f\"{len(history_gru.history['loss']) - lstm_results['num_epochs_trained']:+d}\"\n",
                "        ]\n",
                "    })\n",
                "    \n",
                "    print(\"\\n\" + \"=\"*80)\n",
                "    print(\" \"*30 + \"LSTM vs GRU\")\n",
                "    print(\"=\"*80)\n",
                "    print(comparison_df.to_string(index=False))\n",
                "    print(\"=\"*80)\n",
                "    \n",
                "    # Guardarecomparaci√≥n\n",
                "    comparison_df.to_csv('models/lstm_vs_gru_comparison.csv', index=False)\n",
                "    \n",
                "except Exception as e:\n",
                "    print(f\"‚ö†Ô∏è No se pudo comparar con LSTM: {e}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "visualize",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualizaci√≥n de predicciones\n",
                "test_dates = df_timeseries['fecha'].iloc[SEQ_LENGTH+train_size+val_size:]\n",
                "\n",
                "fig, axes = plt.subplots(2, 1, figsize=(16, 10))\n",
                "\n",
                "# Plot 1: Predicciones GRU\n",
                "axes[0].plot(test_dates, y_test_gru_inv, linewidth=2, label='Real', color='steelblue')\n",
                "axes[0].plot(test_dates, y_pred_gru_inv, linewidth=2, label='Predicci√≥n GRU', \n",
                "             color='coral', linestyle='--')\n",
                "axes[0].fill_between(test_dates, y_test_gru_inv.flatten(), y_pred_gru_inv.flatten(), \n",
                "                      alpha=0.2, color='gray')\n",
                "axes[0].set_ylabel('Total de Delitos')\n",
                "axes[0].set_title(f'GRU - Predicciones (MAE: {gru_metrics[0]:.2f}, R¬≤: {gru_metrics[2]:.4f})', \n",
                "                  fontsize=12, fontweight='bold')\n",
                "axes[0].legend()\n",
                "axes[0].grid(True, alpha=0.3)\n",
                "\n",
                "# Plot 2: Comparaci√≥n de errores (si hay LSTM)\n",
                "try:\n",
                "    lstm_preds = np.load('models/lstm_predictions_test.npy')\n",
                "    \n",
                "    error_lstm = np.abs(y_test_gru_inv.flatten() - lstm_preds.flatten())\n",
                "    error_gru = np.abs(y_test_gru_inv.flatten() - y_pred_gru_inv.flatten())\n",
                "    \n",
                "    axes[1].plot(test_dates, error_lstm, label='Error LSTM', linewidth=2, alpha=0.7)\n",
                "    axes[1].plot(test_dates, error_gru, label='Error GRU', linewidth=2, alpha=0.7)\n",
                "    axes[1].set_xlabel('Fecha')\n",
                "    axes[1].set_ylabel('Error Absoluto')\n",
                "    axes[1].set_title('Comparaci√≥n de Errores: LSTM vs GRU', fontsize=12, fontweight='bold')\n",
                "    axes[1].legend()\n",
                "    axes[1].grid(True, alpha=0.3)\n",
                "except:\n",
                "    # Solo errores de GRU\n",
                "    error_gru = np.abs(y_test_gru_inv.flatten() - y_pred_gru_inv.flatten())\n",
                "    axes[1].plot(test_dates, error_gru, label='Error GRU', linewidth=2, color='coral')\n",
                "    axes[1].set_xlabel('Fecha')\n",
                "    axes[1].set_ylabel('Error Absoluto')\n",
                "    axes[1].set_title('Error de Predicci√≥n GRU', fontsize=12, fontweight='bold')\n",
                "    axes[1].legend()\n",
                "    axes[1].grid(True, alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('models/gru_predictions_comparison.png', dpi=300, bbox_inches='tight')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "save",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Guardar modelo y resultados\n",
                "print(\"üíæ Guardando modelo GRU...\\n\")\n",
                "\n",
                "gru_model.save('models/gru_predictor_final.keras')\n",
                "\n",
                "with open('models/gru_scaler.pkl', 'wb') as f:\n",
                "    pickle.dump(scaler_gru, f)\n",
                "\n",
                "with open('models/gru_history.pkl', 'wb') as f:\n",
                "    pickle.dump(history_gru.history, f)\n",
                "\n",
                "results_gru = {\n",
                "    'test_mae': gru_metrics[0],\n",
                "    'test_rmse': gru_metrics[1],\n",
                "    'test_r2': gru_metrics[2],\n",
                "    'test_mape': gru_metrics[3],\n",
                "    'num_parameters': total_params_gru,\n",
                "    'training_time_seconds': training_time_gru,\n",
                "    'num_epochs_trained': len(history_gru.history['loss'])\n",
                "}\n",
                "\n",
                "with open('models/gru_results.pkl', 'wb') as f:\n",
                "    pickle.dump(results_gru, f)\n",
                "\n",
                "print(\"‚úÖ Modelo GRU guardado\")\n",
                "print(\"\\nüìù Pr√≥ximo paso: CNN para an√°lisis espacial\")\n",
                "print(\"   ‚Üí Notebook: 05_CNN_Spatial.ipynb\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}