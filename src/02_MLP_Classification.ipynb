{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "header",
            "metadata": {},
            "source": [
                "# üß† Multi-Layer Perceptron (MLP) - Clasificaci√≥n de Delitos\n",
                "## Red Neuronal Feedforward para Clasificaci√≥n Multi-clase\n",
                "\n",
                "---\n",
                "\n",
                "### Objetivos:\n",
                "1. Construir y entrenar una red MLP para clasificar tipos de delitos\n",
                "2. Optimizar hiperpar√°metros (capas, neuronas, regularizaci√≥n)\n",
                "3. Evaluar rendimiento con m√©tricas detalladas\n",
                "4. Visualizar matriz de confusi√≥n y curvas de aprendizaje\n",
                "5. Interpretar resultados y features importantes\n",
                "\n",
                "**Autor**: Adonnay Bazaldua  \n",
                "**Fecha**: Noviembre 2025"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "intro",
            "metadata": {},
            "source": [
                "## 1. Importaci√≥n de Librer√≠as"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "imports",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Deep Learning\n",
                "import tensorflow as tf\n",
                "from tensorflow import keras\n",
                "from tensorflow.keras import layers, models, callbacks\n",
                "from tensorflow.keras.utils import to_categorical\n",
                "\n",
                "# Procesamiento de datos\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import pickle\n",
                "\n",
                "# M√©tricas y evaluaci√≥n\n",
                "from sklearn.metrics import classification_report, confusion_matrix\n",
                "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
                "from sklearn.metrics import roc_auc_score, roc_curve\n",
                "\n",
                "# Visualizaci√≥n\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import plotly.express as px\n",
                "import plotly.graph_objects as go\n",
                "from plotly.subplots import make_subplots\n",
                "\n",
                "# Utils\n",
                "import os\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# Set random seeds\n",
                "np.random.seed(42)\n",
                "tf.random.set_seed(42)\n",
                "\n",
                "print(f\"‚úÖ TensorFlow version: {tf.__version__}\")\n",
                "print(f\"‚úÖ GPU disponible: {tf.config.list_physical_devices('GPU')}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "load_data",
            "metadata": {},
            "source": [
                "## 2. Carga de Datos Preprocesados"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "load",
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"üìÇ Cargando datos preprocesados...\\n\")\n",
                "\n",
                "# Cargar conjuntos de datos\n",
                "X_train = np.load('processed_data/X_train.npy')\n",
                "X_val = np.load('processed_data/X_val.npy')\n",
                "X_test = np.load('processed_data/X_test.npy')\n",
                "y_train = np.load('processed_data/y_train.npy')\n",
                "y_val = np.load('processed_data/y_val.npy')\n",
                "y_test = np.load('processed_data/y_test.npy')\n",
                "\n",
                "# Cargar metadata\n",
                "with open('processed_data/metadata.pkl', 'rb') as f:\n",
                "    metadata = pickle.load(f)\n",
                "\n",
                "# Cargar encoders\n",
                "with open('processed_data/target_encoder.pkl', 'rb') as f:\n",
                "    target_encoder = pickle.load(f)\n",
                "\n",
                "with open('processed_data/feature_names.pkl', 'rb') as f:\n",
                "    feature_names = pickle.load(f)\n",
                "\n",
                "# Informaci√≥n\n",
                "num_classes = metadata['num_classes']\n",
                "num_features = metadata['num_features']\n",
                "\n",
                "print(f\"‚úÖ Datos cargados:\")\n",
                "print(f\"   Train: {X_train.shape}\")\n",
                "print(f\"   Val:   {X_val.shape}\")\n",
                "print(f\"   Test:  {X_test.shape}\")\n",
                "print(f\"\\n   Features: {num_features}\")\n",
                "print(f\"   Clases: {num_classes}\")\n",
                "print(f\"\\n   Clases disponibles (primeras 10):\")\n",
                "for i, clase in enumerate(target_encoder.classes_[:10], 1):\n",
                "    print(f\"   {i:2d}. {clase}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "one_hot",
            "metadata": {},
            "source": [
                "## 3. Codificaci√≥n One-Hot de Etiquetas\n",
                "\n",
                "Para clasificaci√≥n multi-clase con softmax, necesitamos one-hot encoding."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "encode_labels",
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"üè∑Ô∏è  Codificando etiquetas con One-Hot...\\n\")\n",
                "\n",
                "# Convertir a one-hot encoding\n",
                "y_train_cat = to_categorical(y_train, num_classes=num_classes)\n",
                "y_val_cat = to_categorical(y_val, num_classes=num_classes)\n",
                "y_test_cat = to_categorical(y_test, num_classes=num_classes)\n",
                "\n",
                "print(f\"‚úÖ One-Hot Encoding aplicado:\")\n",
                "print(f\"   y_train: {y_train.shape} ‚Üí {y_train_cat.shape}\")\n",
                "print(f\"   y_val:   {y_val.shape} ‚Üí {y_val_cat.shape}\")\n",
                "print(f\"   y_test:  {y_test.shape} ‚Üí {y_test_cat.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "model_architecture",
            "metadata": {},
            "source": [
                "## 4. Construcci√≥n de la Arquitectura MLP\n",
                "\n",
                "### Arquitectura propuesta:\n",
                "```\n",
                "Input(num_features) \n",
                "  ‚Üí Dense(256) ‚Üí ReLU ‚Üí BatchNorm ‚Üí Dropout(0.3)\n",
                "  ‚Üí Dense(128) ‚Üí ReLU ‚Üí BatchNorm ‚Üí Dropout(0.3)\n",
                "  ‚Üí Dense(64)  ‚Üí ReLU ‚Üí BatchNorm ‚Üí Dropout(0.2)\n",
                "  ‚Üí Dense(num_classes) ‚Üí Softmax\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "build_model",
            "metadata": {},
            "outputs": [],
            "source": [
                "def create_mlp_model(input_dim, num_classes, learning_rate=0.001):\n",
                "    \"\"\"\n",
                "    Crea un modelo MLP para clasificaci√≥n multi-clase.\n",
                "    \n",
                "    Args:\n",
                "        input_dim: N√∫mero de features de entrada\n",
                "        num_classes: N√∫mero de clases a predecir\n",
                "        learning_rate: Tasa de aprendizaje\n",
                "    \n",
                "    Returns:\n",
                "        Modelo MLP compilado\n",
                "    \"\"\"\n",
                "    model = models.Sequential([\n",
                "        # Input layer\n",
                "        layers.Input(shape=(input_dim,)),\n",
                "        \n",
                "        # Hidden layer 1\n",
                "        layers.Dense(256, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001)),\n",
                "        layers.BatchNormalization(),\n",
                "        layers.Dropout(0.3),\n",
                "        \n",
                "        # Hidden layer 2\n",
                "        layers.Dense(128, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001)),\n",
                "        layers.BatchNormalization(),\n",
                "        layers.Dropout(0.3),\n",
                "        \n",
                "        # Hidden layer 3\n",
                "        layers.Dense(64, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001)),\n",
                "        layers.BatchNormalization(),\n",
                "        layers.Dropout(0.2),\n",
                "        \n",
                "        # Output layer\n",
                "        layers.Dense(num_classes, activation='softmax')\n",
                "    ], name='MLP_Crime_Classifier')\n",
                "    \n",
                "    # Compilar modelo\n",
                "    model.compile(\n",
                "        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
                "        loss='categorical_crossentropy',\n",
                "        metrics=['accuracy', keras.metrics.TopKCategoricalAccuracy(k=3, name='top_3_accuracy')]\n",
                "    )\n",
                "    \n",
                "    return model\n",
                "\n",
                "# Crear modelo\n",
                "print(\"üèóÔ∏è  Construyendo modelo MLP...\\n\")\n",
                "mlp_model = create_mlp_model(input_dim=num_features, num_classes=num_classes)\n",
                "\n",
                "# Resumen del modelo\n",
                "mlp_model.summary()\n",
                "\n",
                "# Contar par√°metros\n",
                "total_params = mlp_model.count_params()\n",
                "print(f\"\\nüìä Total de par√°metros: {total_params:,}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "callbacks",
            "metadata": {},
            "source": [
                "## 5. Configuraci√≥n de Callbacks"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "setup_callbacks",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Crear directorio para modelos\n",
                "os.makedirs('models', exist_ok=True)\n",
                "\n",
                "# Callbacks\n",
                "early_stopping = callbacks.EarlyStopping(\n",
                "    monitor='val_loss',\n",
                "    patience=10,\n",
                "    restore_best_weights=True,\n",
                "    verbose=1\n",
                ")\n",
                "\n",
                "reduce_lr = callbacks.ReduceLROnPlateau(\n",
                "    monitor='val_loss',\n",
                "    factor=0.5,\n",
                "    patience=5,\n",
                "    min_lr=1e-7,\n",
                "    verbose=1\n",
                ")\n",
                "\n",
                "model_checkpoint = callbacks.ModelCheckpoint(\n",
                "    'models/mlp_best.keras',\n",
                "    monitor='val_accuracy',\n",
                "    save_best_only=True,\n",
                "    verbose=1\n",
                ")\n",
                "\n",
                "tensorboard_callback = callbacks.TensorBoard(\n",
                "    log_dir='logs/mlp',\n",
                "    histogram_freq=1\n",
                ")\n",
                "\n",
                "callbacks_list = [early_stopping, reduce_lr, model_checkpoint, tensorboard_callback]\n",
                "\n",
                "print(\"‚úÖ Callbacks configurados:\")\n",
                "print(\"   - EarlyStopping (patience=10)\")\n",
                "print(\"   - ReduceLROnPlateau (patience=5, factor=0.5)\")\n",
                "print(\"   - ModelCheckpoint (mejor val_accuracy)\")\n",
                "print(\"   - TensorBoard (logs/mlp)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "training",
            "metadata": {},
            "source": [
                "## 6. Entrenamiento del Modelo"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "train",
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"üöÄ Iniciando entrenamiento...\\n\")\n",
                "\n",
                "# Par√°metros de entrenamiento\n",
                "BATCH_SIZE = 128\n",
                "EPOCHS = 100\n",
                "\n",
                "# Entrenar modelo\n",
                "history = mlp_model.fit(\n",
                "    X_train, y_train_cat,\n",
                "    batch_size=BATCH_SIZE,\n",
                "    epochs=EPOCHS,\n",
                "    validation_data=(X_val, y_val_cat),\n",
                "    callbacks=callbacks_list,\n",
                "    verbose=1\n",
                ")\n",
                "\n",
                "print(\"\\n‚úÖ Entrenamiento completado!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "learning_curves",
            "metadata": {},
            "source": [
                "## 7. Visualizaci√≥n de Curvas de Aprendizaje"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "plot_learning",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Crear visualizaciones\n",
                "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
                "\n",
                "# Loss\n",
                "axes[0, 0].plot(history.history['loss'], label='Train Loss', linewidth=2)\n",
                "axes[0, 0].plot(history.history['val_loss'], label='Val Loss', linewidth=2)\n",
                "axes[0, 0].set_xlabel('√âpoca')\n",
                "axes[0, 0].set_ylabel('Loss (Categorical Crossentropy)')\n",
                "axes[0, 0].set_title('Curva de P√©rdida')\n",
                "axes[0, 0].legend()\n",
                "axes[0, 0].grid(True, alpha=0.3)\n",
                "\n",
                "# Accuracy\n",
                "axes[0, 1].plot(history.history['accuracy'], label='Train Accuracy', linewidth=2)\n",
                "axes[0, 1].plot(history.history['val_accuracy'], label='Val Accuracy', linewidth=2)\n",
                "axes[0, 1].set_xlabel('√âpoca')\n",
                "axes[0, 1].set_ylabel('Accuracy')\n",
                "axes[0, 1].set_title('Curva de Precisi√≥n')\n",
                "axes[0, 1].legend()\n",
                "axes[0, 1].grid(True, alpha=0.3)\n",
                "\n",
                "# Top-3 Accuracy\n",
                "axes[1, 0].plot(history.history['top_3_accuracy'], label='Train Top-3', linewidth=2)\n",
                "axes[1, 0].plot(history.history['val_top_3_accuracy'], label='Val Top-3', linewidth=2)\n",
                "axes[1, 0].set_xlabel('√âpoca')\n",
                "axes[1, 0].set_ylabel('Top-3 Accuracy')\n",
                "axes[1, 0].set_title('Top-3 Categorical Accuracy')\n",
                "axes[1, 0].legend()\n",
                "axes[1, 0].grid(True, alpha=0.3)\n",
                "\n",
                "# Learning Rate (si se redujo)\n",
                "if 'lr' in history.history:\n",
                "    axes[1, 1].plot(history.history['lr'], linewidth=2, color='red')\n",
                "    axes[1, 1].set_xlabel('√âpoca')\n",
                "    axes[1, 1].set_ylabel('Learning Rate')\n",
                "    axes[1, 1].set_title('Learning Rate Schedule')\n",
                "    axes[1, 1].set_yscale('log')\n",
                "    axes[1, 1].grid(True, alpha=0.3)\n",
                "else:\n",
                "    # Comparaci√≥n directa Train vs Val Accuracy\n",
                "    epochs_range = range(1, len(history.history['accuracy']) + 1)\n",
                "    axes[1, 1].plot(epochs_range, history.history['accuracy'], 'b-', label='Train', linewidth=2)\n",
                "    axes[1, 1].plot(epochs_range, history.history['val_accuracy'], 'r-', label='Val', linewidth=2)\n",
                "    axes[1, 1].fill_between(epochs_range, history.history['accuracy'], \n",
                "                            history.history['val_accuracy'], alpha=0.2)\n",
                "    axes[1, 1].set_xlabel('√âpoca')\n",
                "    axes[1, 1].set_ylabel('Accuracy')\n",
                "    axes[1, 1].set_title('Overfitting Analysis')\n",
                "    axes[1, 1].legend()\n",
                "    axes[1, 1].grid(True, alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('models/mlp_training_curves.png', dpi=300, bbox_inches='tight')\n",
                "plt.show()\n",
                "\n",
                "print(\"‚úÖ Curvas de aprendizaje guardadas en 'models/mlp_training_curves.png'\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "evaluation",
            "metadata": {},
            "source": [
                "## 8. Evaluaci√≥n en Conjunto de Prueba"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "evaluate",
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"üìä Evaluando modelo en conjunto de prueba...\\n\")\n",
                "\n",
                "# Evaluar\n",
                "test_loss, test_accuracy, test_top3_accuracy = mlp_model.evaluate(X_test, y_test_cat, verbose=0)\n",
                "\n",
                "print(f\"\\nüéØ Resultados en Test Set:\")\n",
                "print(f\"   Loss: {test_loss:.4f}\")\n",
                "print(f\"   Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
                "print(f\"   Top-3 Accuracy: {test_top3_accuracy:.4f} ({test_top3_accuracy*100:.2f}%)\")\n",
                "\n",
                "# Predicciones\n",
                "y_pred_proba = mlp_model.predict(X_test, verbose=0)\n",
                "y_pred = np.argmax(y_pred_proba, axis=1)\n",
                "\n",
                "# M√©tricas detalladas\n",
                "print(f\"\\nüìã Reporte de Clasificaci√≥n (Top 10 clases m√°s frecuentes):\\n\")\n",
                "\n",
                "# Obtener las 10 clases m√°s frecuentes en el conjunto de prueba\n",
                "unique, counts = np.unique(y_test, return_counts=True)\n",
                "top_10_indices = unique[np.argsort(counts)[-10:]]\n",
                "\n",
                "# Filtrar predicciones y real para top 10\n",
                "mask = np.isin(y_test, top_10_indices)\n",
                "y_test_top10 = y_test[mask]\n",
                "y_pred_top10 = y_pred[mask]\n",
                "\n",
                "# Reporte de clasificaci√≥n para top 10\n",
                "top10_class_names = [target_encoder.classes_[i] for i in top_10_indices]\n",
                "print(classification_report(y_test_top10, y_pred_top10, \n",
                "                          labels=top_10_indices,\n",
                "                          target_names=top10_class_names,\n",
                "                          digits=3))"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "confusion_matrix",
            "metadata": {},
            "source": [
                "## 9. Matriz de Confusi√≥n (Top 10 Clases)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "plot_confusion",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Calcular matriz de confusi√≥n para top 10\n",
                "cm = confusion_matrix(y_test_top10, y_pred_top10, labels=top_10_indices)\n",
                "\n",
                "# Normalizar por filas (recall)\n",
                "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
                "\n",
                "# Visualizar\n",
                "fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
                "\n",
                "# Matriz absoluta\n",
                "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
                "            xticklabels=[c[:30] for c in top10_class_names],\n",
                "            yticklabels=[c[:30] for c in top10_class_names],\n",
                "            ax=axes[0], cbar_kws={'label': 'N√∫mero de muestras'})\n",
                "axes[0].set_title('Matriz de Confusi√≥n (Valores Absolutos)\\nTop 10 Clases', fontsize=12, fontweight='bold')\n",
                "axes[0].set_xlabel('Predicci√≥n')\n",
                "axes[0].set_ylabel('Real')\n",
                "axes[0].tick_params(axis='x', rotation=45)\n",
                "axes[0].tick_params(axis='y', rotation=0)\n",
                "\n",
                "# Matriz normalizada\n",
                "sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='RdYlGn',\n",
                "            xticklabels=[c[:30] for c in top10_class_names],\n",
                "            yticklabels=[c[:30] for c in top10_class_names],\n",
                "            ax=axes[1], cbar_kws={'label': 'Proporci√≥n'})\n",
                "axes[1].set_title('Matriz de Confusi√≥n Normalizada (Recall)\\nTop 10 Clases', fontsize=12, fontweight='bold')\n",
                "axes[1].set_xlabel('Predicci√≥n')\n",
                "axes[1].set_ylabel('Real')\n",
                "axes[1].tick_params(axis='x', rotation=45)\n",
                "axes[1].tick_params(axis='y', rotation=0)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('models/mlp_confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
                "plt.show()\n",
                "\n",
                "print(\"‚úÖ Matriz de confusi√≥n guardada en 'models/mlp_confusion_matrix.png'\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "feature_importance",
            "metadata": {},
            "source": [
                "## 10. An√°lisis de Importancia de Features"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "analyze_features",
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"üîç Analizando importancia de features...\\n\")\n",
                "\n",
                "# Obtener pesos de la primera capa\n",
                "first_layer_weights = mlp_model.layers[0].get_weights()[0]  # Shape: (num_features, 256)\n",
                "\n",
                "# Calcular importancia como magnitud promedio de los pesos\n",
                "feature_importance = np.abs(first_layer_weights).mean(axis=1)\n",
                "\n",
                "# Crear DataFrame\n",
                "importance_df = pd.DataFrame({\n",
                "    'Feature': feature_names,\n",
                "    'Importance': feature_importance\n",
                "}).sort_values('Importance', ascending=False)\n",
                "\n",
                "# Top 20 features\n",
                "top_20_features = importance_df.head(20)\n",
                "\n",
                "# Visualizar\n",
                "fig, ax = plt.subplots(figsize=(12, 8))\n",
                "bars = ax.barh(range(len(top_20_features)), top_20_features['Importance'].values)\n",
                "\n",
                "# Colorear por tipo\n",
                "colors = []\n",
                "for feat in top_20_features['Feature']:\n",
                "    if any(t in feat for t in ['latitud', 'longitud', 'densidad']):\n",
                "        colors.append('steelblue')\n",
                "    elif any(t in feat for t in ['a√±o', 'mes', 'dia', 'hora', 'semana', 'trimestre', 'sin', 'cos']):\n",
                "        colors.append('coral')\n",
                "    else:\n",
                "        colors.append('lightgreen')\n",
                "\n",
                "for bar, color in zip(bars, colors):\n",
                "    bar.set_color(color)\n",
                "\n",
                "ax.set_yticks(range(len(top_20_features)))\n",
                "ax.set_yticklabels(top_20_features['Feature'].values, fontsize=9)\n",
                "ax.set_xlabel('Importancia (Magnitud Promedio de Pesos)', fontsize=10)\n",
                "ax.set_title('Top 20 Features M√°s Importantes\\n(MLP - Primera Capa)', \n",
                "             fontsize=12, fontweight='bold')\n",
                "ax.invert_yaxis()\n",
                "ax.grid(True, alpha=0.3, axis='x')\n",
                "\n",
                "# Leyenda\n",
                "from matplotlib.patches import Patch\n",
                "legend_elements = [\n",
                "    Patch(facecolor='steelblue', label='Geogr√°ficas'),\n",
                "    Patch(facecolor='coral', label='Temporales'),\n",
                "    Patch(facecolor='lightgreen', label='Categ√≥ricas')\n",
                "]\n",
                "ax.legend(handles=legend_elements, loc='lower right')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('models/mlp_feature_importance.png', dpi=300, bbox_inches='tight')\n",
                "plt.show()\n",
                "\n",
                "print(\"‚úÖ An√°lisis de importancia guardado en 'models/mlp_feature_importance.png'\")\n",
                "print(\"\\nüìä Top 10 Features m√°s importantes:\")\n",
                "for i, row in top_20_features.head(10).iterrows():\n",
                "    print(f\"   {row['Feature']:30s}: {row['Importance']:.6f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "save_model",
            "metadata": {},
            "source": [
                "## 11. Guardar Modelo Final y Resultados"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "save",
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"üíæ Guardando modelo y resultados...\\n\")\n",
                "\n",
                "# Guardar modelo completo\n",
                "mlp_model.save('models/mlp_classifier_final.keras')\n",
                "\n",
                "# Guardar historial de entrenamiento\n",
                "with open('models/mlp_history.pkl', 'wb') as f:\n",
                "    pickle.dump(history.history, f)\n",
                "\n",
                "# Guardar predicciones\n",
                "np.save('models/mlp_predictions_test.npy', y_pred)\n",
                "np.save('models/mlp_probabilities_test.npy', y_pred_proba)\n",
                "\n",
                "# Guardar resultados\n",
                "results = {\n",
                "    'test_loss': test_loss,\n",
                "    'test_accuracy': test_accuracy,\n",
                "    'test_top3_accuracy': test_top3_accuracy,\n",
                "    'num_parameters': total_params,\n",
                "    'num_epochs_trained': len(history.history['loss']),\n",
                "    'best_val_accuracy': max(history.history['val_accuracy']),\n",
                "    'feature_importance': importance_df.to_dict()\n",
                "}\n",
                "\n",
                "with open('models/mlp_results.pkl', 'wb') as f:\n",
                "    pickle.dump(results, f)\n",
                "\n",
                "print(\"‚úÖ Archivos guardados:\")\n",
                "print(\"   - models/mlp_classifier_final.keras\")\n",
                "print(\"   - models/mlp_best.keras\")\n",
                "print(\"   - models/mlp_history.pkl\")\n",
                "print(\"   - models/mlp_predictions_test.npy\")\n",
                "print(\"   - models/mlp_probabilities_test.npy\")\n",
                "print(\"   - models/mlp_results.pkl\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "summary",
            "metadata": {},
            "source": [
                "## 12. Resumen Final"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "final_summary",
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\"*80)\n",
                "print(\" \"*25 + \"RESUMEN DEL MODELO MLP\")\n",
                "print(\"=\"*80)\n",
                "\n",
                "print(f\"\\nüèóÔ∏è  ARQUITECTURA:\")\n",
                "print(f\"   Tipo: Multi-Layer Perceptron (Feedforward)\")\n",
                "print(f\"   Capas ocultas: 3 (256 ‚Üí 128 ‚Üí 64 neuronas)\")\n",
                "print(f\"   Funci√≥n activaci√≥n: ReLU\")\n",
                "print(f\"   Regularizaci√≥n: L2 + Dropout + BatchNormalization\")\n",
                "print(f\"   Par√°metros totales: {total_params:,}\")\n",
                "\n",
                "print(f\"\\nüìä DATOS:\")\n",
                "print(f\"   Features de entrada: {num_features}\")\n",
                "print(f\"   Clases de salida: {num_classes}\")\n",
                "print(f\"   Muestras entrenamiento: {len(X_train):,}\")\n",
                "print(f\"   Muestras validaci√≥n: {len(X_val):,}\")\n",
                "print(f\"   Muestras prueba: {len(X_test):,}\")\n",
                "\n",
                "print(f\"\\nüéØ RENDIMIENTO:\")\n",
                "print(f\"   Test Accuracy: {test_accuracy*100:.2f}%\")\n",
                "print(f\"   Test Top-3 Accuracy: {test_top3_accuracy*100:.2f}%\")\n",
                "print(f\"   Test Loss: {test_loss:.4f}\")\n",
                "print(f\"   Mejor Val Accuracy: {max(history.history['val_accuracy'])*100:.2f}%\")\n",
                "print(f\"   √âpocas entrenadas: {len(history.history['loss'])}\")\n",
                "\n",
                "print(f\"\\nüîù TOP 3 FEATURES M√ÅS IMPORTANTES:\")\n",
                "for i, row in importance_df.head(3).iterrows():\n",
                "    print(f\"   {i+1}. {row['Feature']}: {row['Importance']:.6f}\")\n",
                "\n",
                "print(f\"\\n‚úÖ MODELO MLP COMPLETADO Y GUARDADO\")\n",
                "print(\"\\n\" + \"=\"*80)\n",
                "\n",
                "print(\"\\nüìù Pr√≥ximo paso: Implementar LSTM para series temporales\")\n",
                "print(\"   ‚Üí Notebook: 03_LSTM_TimeSeries.ipynb\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}