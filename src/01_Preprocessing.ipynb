{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# üìä Preprocesamiento de Datos - Delitos CDMX\n",
    "## Preparaci√≥n de Datos para Redes Neuronales\n",
    "\n",
    "---\n",
    "\n",
    "### Objetivos:\n",
    "1. Cargar datos desde formato Parquet\n",
    "2. An√°lisis de calidad de datos\n",
    "3. Feature Engineering (caracter√≠sticas temporales, geogr√°ficas, categ√≥ricas)\n",
    "4. Normalizaci√≥n y codificaci√≥n\n",
    "5. Divisi√≥n de datasets (train/validation/test)\n",
    "6. Guardar datos procesados para modelos de redes neuronales\n",
    "\n",
    "**Autor**: Adonnay Bazaldua  \n",
    "**Fecha**: Noviembre 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports",
   "metadata": {},
   "source": [
    "## 1. Importaci√≥n de Librer√≠as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "import_libs",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Procesamiento de datos\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# PySpark para grandes vol√∫menes\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Scikit-learn para preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Visualizaci√≥n\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Configuraci√≥n\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úÖ Librer√≠as importadas correctamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spark_init",
   "metadata": {},
   "source": [
    "## 2. Inicializaci√≥n de Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spark_session",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear SparkSession optimizada\n",
    "spark = (SparkSession.builder\n",
    "         .appName(\"Neural_Networks_CDMX_Preprocessing\")\n",
    "         .master(\"local[4]\")\n",
    "         .config(\"spark.driver.memory\", \"4g\")\n",
    "         .config(\"spark.executor.memory\", \"4g\")\n",
    "         .config(\"spark.sql.shuffle.partitions\", \"8\")\n",
    "         .config(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "         .getOrCreate())\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "print(f\"‚úÖ Spark Session iniciada: {spark.version}\")\n",
    "print(f\"   Cores disponibles: {spark.sparkContext.defaultParallelism}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load_data",
   "metadata": {},
   "source": [
    "## 3. Carga de Datos desde Parquet\n",
    "\n",
    "Los datos est√°n particionados por a√±o para optimizar las consultas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_parquet",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruta del dataset\n",
    "parquet_path = \"delitos_cdmx.parquet\"\n",
    "\n",
    "# Cargar datos\n",
    "print(\"üìÇ Cargando datos desde Parquet...\")\n",
    "df_spark = spark.read.parquet(parquet_path)\n",
    "\n",
    "# Informaci√≥n b√°sica\n",
    "print(f\"\\nüìä Dataset cargado:\")\n",
    "print(f\"   Registros: {df_spark.count():,}\")\n",
    "print(f\"   Columnas: {len(df_spark.columns)}\")\n",
    "print(f\"   Particiones: {df_spark.rdd.getNumPartitions()}\")\n",
    "\n",
    "# Mostrar esquema\n",
    "print(\"\\nüîç Esquema del dataset:\")\n",
    "df_spark.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data_quality",
   "metadata": {},
   "source": [
    "## 4. An√°lisis de Calidad de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "check_nulls",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analizar valores nulos\n",
    "print(\"üîé An√°lisis de valores nulos:\\n\")\n",
    "\n",
    "null_counts = df_spark.select([\n",
    "    F.count(F.when(F.col(c).isNull(), c)).alias(c) \n",
    "    for c in df_spark.columns\n",
    "]).collect()[0].asDict()\n",
    "\n",
    "total_rows = df_spark.count()\n",
    "\n",
    "null_df = pd.DataFrame([\n",
    "    {\n",
    "        'Columna': col,\n",
    "        'Nulos': null_counts[col],\n",
    "        'Porcentaje': (null_counts[col] / total_rows) * 100\n",
    "    }\n",
    "    for col in df_spark.columns\n",
    "])\n",
    "\n",
    "null_df = null_df.sort_values('Nulos', ascending=False)\n",
    "null_df_filtered = null_df[null_df['Nulos'] > 0]\n",
    "\n",
    "if len(null_df_filtered) > 0:\n",
    "    print(null_df_filtered.to_string(index=False))\n",
    "else:\n",
    "    print(\"‚úÖ No hay valores nulos en el dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sample_data",
   "metadata": {},
   "source": [
    "## 5. Muestra de Datos\n",
    "\n",
    "Para trabajar eficientemente, crearemos una muestra estratificada del dataset completo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create_sample",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir a Pandas para an√°lisis m√°s r√°pido\n",
    "# Tomar muestra estratificada (20% de datos, ~420k registros)\n",
    "print(\"üìä Creando muestra estratificada...\")\n",
    "\n",
    "# Filtrar registros con coordenadas v√°lidas (importantes para an√°lisis espacial)\n",
    "df_filtered = df_spark.filter(\n",
    "    (F.col(\"latitud\").isNotNull()) & \n",
    "    (F.col(\"longitud\").isNotNull()) &\n",
    "    (F.col(\"fecha_hecho\").isNotNull())\n",
    ")\n",
    "\n",
    "print(f\"   Registros con coordenadas v√°lidas: {df_filtered.count():,}\")\n",
    "\n",
    "# Tomar muestra\n",
    "sample_fraction = 0.20\n",
    "df_sample = df_filtered.sample(withReplacement=False, fraction=sample_fraction, seed=42)\n",
    "\n",
    "# Convertir a Pandas\n",
    "df = df_sample.toPandas()\n",
    "\n",
    "print(f\"‚úÖ Muestra creada: {len(df):,} registros ({sample_fraction*100}%)\")\n",
    "print(f\"\\nüìã Informaci√≥n del DataFrame:\")\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feature_engineering",
   "metadata": {},
   "source": [
    "## 6. Feature Engineering\n",
    "\n",
    "### 6.1 Features Temporales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "temporal_features",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üïê Creando features temporales...\\n\")\n",
    "\n",
    "# Asegurar que fecha_hecho es datetime\n",
    "df['fecha_hecho'] = pd.to_datetime(df['fecha_hecho'])\n",
    "\n",
    "# Extraer componentes temporales\n",
    "df['a√±o'] = df['fecha_hecho'].dt.year\n",
    "df['mes'] = df['fecha_hecho'].dt.month\n",
    "df['dia'] = df['fecha_hecho'].dt.day\n",
    "df['dia_semana'] = df['fecha_hecho'].dt.dayofweek  # 0=Lunes, 6=Domingo\n",
    "df['trimestre'] = df['fecha_hecho'].dt.quarter\n",
    "df['semana_a√±o'] = df['fecha_hecho'].dt.isocalendar().week\n",
    "\n",
    "# Caracter√≠sticas c√≠clicas (importante para redes neuronales)\n",
    "df['mes_sin'] = np.sin(2 * np.pi * df['mes'] / 12)\n",
    "df['mes_cos'] = np.cos(2 * np.pi * df['mes'] / 12)\n",
    "df['dia_semana_sin'] = np.sin(2 * np.pi * df['dia_semana'] / 7)\n",
    "df['dia_semana_cos'] = np.cos(2 * np.pi * df['dia_semana'] / 7)\n",
    "\n",
    "# Hora del d√≠a (si est√° disponible)\n",
    "if 'hora_hecho' in df.columns and df['hora_hecho'].notna().any():\n",
    "    # Extraer hora como n√∫mero\n",
    "    df['hora'] = pd.to_datetime(df['hora_hecho'], format='%H:%M:%S', errors='coerce').dt.hour\n",
    "    df['hora_sin'] = np.sin(2 * np.pi * df['hora'] / 24)\n",
    "    df['hora_cos'] = np.cos(2 * np.pi * df['hora'] / 24)\n",
    "    \n",
    "    # Categorizar por periodo del d√≠a\n",
    "    df['periodo_dia'] = pd.cut(df['hora'], \n",
    "                                bins=[0, 6, 12, 18, 24], \n",
    "                                labels=['Madrugada', 'Ma√±ana', 'Tarde', 'Noche'],\n",
    "                                include_lowest=True)\n",
    "\n",
    "# Es fin de semana?\n",
    "df['es_fin_semana'] = (df['dia_semana'] >= 5).astype(int)\n",
    "\n",
    "print(\"‚úÖ Features temporales creadas:\")\n",
    "temporal_features = ['a√±o', 'mes', 'dia', 'dia_semana', 'trimestre', 'semana_a√±o',\n",
    "                     'mes_sin', 'mes_cos', 'dia_semana_sin', 'dia_semana_cos',\n",
    "                     'es_fin_semana']\n",
    "if 'hora' in df.columns:\n",
    "    temporal_features.extend(['hora', 'hora_sin', 'hora_cos', 'periodo_dia'])\n",
    "    \n",
    "print(f\"   {temporal_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "geo_features",
   "metadata": {},
   "source": [
    "### 6.2 Features Geogr√°ficas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spatial_features",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üó∫Ô∏è  Creando features geogr√°ficas...\\n\")\n",
    "\n",
    "# Normalizar coordenadas (importante para modelos)\n",
    "df['latitud_norm'] = (df['latitud'] - df['latitud'].mean()) / df['latitud'].std()\n",
    "df['longitud_norm'] = (df['longitud'] - df['longitud'].mean()) / df['longitud'].std()\n",
    "\n",
    "# Densidad delictiva por alcald√≠a\n",
    "alcaldia_counts = df['alcaldia_hecho'].value_counts()\n",
    "df['densidad_alcaldia'] = df['alcaldia_hecho'].map(alcaldia_counts)\n",
    "\n",
    "# Normalizar densidad\n",
    "df['densidad_alcaldia_norm'] = (df['densidad_alcaldia'] - df['densidad_alcaldia'].min()) / \\\n",
    "                                (df['densidad_alcaldia'].max() - df['densidad_alcaldia'].min())\n",
    "\n",
    "# Grid espacial (para CNN)\n",
    "# Dividir CDMX en grid de 50x50 celdas\n",
    "lat_bins = 50\n",
    "lon_bins = 50\n",
    "\n",
    "df['lat_bin'] = pd.cut(df['latitud'], bins=lat_bins, labels=False)\n",
    "df['lon_bin'] = pd.cut(df['longitud'], bins=lon_bins, labels=False)\n",
    "\n",
    "print(\"‚úÖ Features geogr√°ficas creadas:\")\n",
    "geo_features = ['latitud', 'longitud', 'latitud_norm', 'longitud_norm', \n",
    "                'densidad_alcaldia', 'densidad_alcaldia_norm', 'lat_bin', 'lon_bin']\n",
    "print(f\"   {geo_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "categorical_encoding",
   "metadata": {},
   "source": [
    "### 6.3 Codificaci√≥n de Variables Categ√≥ricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "encode_categorical",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üè∑Ô∏è  Codificando variables categ√≥ricas...\\n\")\n",
    "\n",
    "# Variables categ√≥ricas principales\n",
    "categorical_cols = ['delito', 'categoria_delito', 'alcaldia_hecho', 'fiscalia']\n",
    "\n",
    "# Filtrar columnas que existen y tienen datos\n",
    "categorical_cols = [col for col in categorical_cols if col in df.columns and df[col].notna().any()]\n",
    "\n",
    "# Label Encoding (para modelos tree-based y como referencia)\n",
    "label_encoders = {}\n",
    "\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    df[f'{col}_encoded'] = le.fit_transform(df[col].fillna('UNKNOWN'))\n",
    "    label_encoders[col] = le\n",
    "    print(f\"   ‚úì {col}: {len(le.classes_)} clases √∫nicas\")\n",
    "\n",
    "# Para la variable objetivo (delito), guardar el encoder\n",
    "target_encoder = label_encoders['delito']\n",
    "num_classes = len(target_encoder.classes_)\n",
    "\n",
    "print(f\"\\nüìä Total de clases de delitos: {num_classes}\")\n",
    "print(f\"\\nüéØ Top 10 delitos m√°s frecuentes:\")\n",
    "print(df['delito'].value_counts().head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "target_distribution",
   "metadata": {},
   "source": [
    "## 7. An√°lisis de Distribuci√≥n de Clases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "class_distribution",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar distribuci√≥n de clases\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Top 15 delitos\n",
    "top_delitos = df['delito'].value_counts().head(15)\n",
    "axes[0].barh(range(len(top_delitos)), top_delitos.values)\n",
    "axes[0].set_yticks(range(len(top_delitos)))\n",
    "axes[0].set_yticklabels(top_delitos.index, fontsize=8)\n",
    "axes[0].set_xlabel('Frecuencia')\n",
    "axes[0].set_title('Top 15 Tipos de Delitos M√°s Frecuentes')\n",
    "axes[0].invert_yaxis()\n",
    "\n",
    "# Distribuci√≥n por alcald√≠a\n",
    "alcaldia_counts = df['alcaldia_hecho'].value_counts()\n",
    "axes[1].bar(range(len(alcaldia_counts)), alcaldia_counts.values)\n",
    "axes[1].set_xticks(range(len(alcaldia_counts)))\n",
    "axes[1].set_xticklabels(alcaldia_counts.index, rotation=45, ha='right', fontsize=8)\n",
    "axes[1].set_ylabel('Frecuencia')\n",
    "axes[1].set_title('Distribuci√≥n de Delitos por Alcald√≠a')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Estad√≠sticas de balance\n",
    "print(\"\\n‚öñÔ∏è  An√°lisis de balance de clases:\")\n",
    "value_counts = df['delito'].value_counts()\n",
    "print(f\"   Clase m√°s frecuente: {value_counts.index[0]} ({value_counts.iloc[0]:,} casos)\")\n",
    "print(f\"   Clase menos frecuente: {value_counts.index[-1]} ({value_counts.iloc[-1]:,} casos)\")\n",
    "print(f\"   Ratio desbalance: {value_counts.iloc[0] / value_counts.iloc[-1]:.2f}:1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feature_selection",
   "metadata": {},
   "source": [
    "## 8. Selecci√≥n de Features para Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "select_features",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üéØ Seleccionando features para modelos de redes neuronales...\\n\")\n",
    "\n",
    "# Features num√©ricas para MLP\n",
    "numerical_features = [\n",
    "    # Temporales\n",
    "    'a√±o', 'mes', 'dia', 'dia_semana', 'trimestre', 'semana_a√±o',\n",
    "    'mes_sin', 'mes_cos', 'dia_semana_sin', 'dia_semana_cos',\n",
    "    'es_fin_semana',\n",
    "    # Geogr√°ficas\n",
    "    'latitud_norm', 'longitud_norm', 'densidad_alcaldia_norm',\n",
    "]\n",
    "\n",
    "# Agregar hora si existe\n",
    "if 'hora' in df.columns:\n",
    "    numerical_features.extend(['hora', 'hora_sin', 'hora_cos'])\n",
    "\n",
    "# Features categ√≥ricas codificadas\n",
    "categorical_encoded = [f'{col}_encoded' for col in categorical_cols if col != 'delito']\n",
    "\n",
    "# Todas las features para el modelo\n",
    "all_features = numerical_features + categorical_encoded\n",
    "\n",
    "# Variable objetivo\n",
    "target = 'delito_encoded'\n",
    "\n",
    "print(f\"‚úÖ Features seleccionadas: {len(all_features)}\")\n",
    "print(f\"   - Num√©ricas: {len(numerical_features)}\")\n",
    "print(f\"   - Categ√≥ricas codificadas: {len(categorical_encoded)}\")\n",
    "print(f\"\\nüìã Lista de features:\")\n",
    "for i, feat in enumerate(all_features, 1):\n",
    "    print(f\"   {i:2d}. {feat}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data_split",
   "metadata": {},
   "source": [
    "## 9. Divisi√≥n de Datos (Train/Validation/Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train_test_split",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"‚úÇÔ∏è  Dividiendo datos en conjuntos de entrenamiento, validaci√≥n y prueba...\\n\")\n",
    "\n",
    "# Preparar X e y\n",
    "X = df[all_features].copy()\n",
    "y = df[target].copy()\n",
    "\n",
    "# Eliminar posibles NaN\n",
    "X = X.fillna(0)\n",
    "\n",
    "# Primera divisi√≥n: 80% train+val, 20% test\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Segunda divisi√≥n: 75% train, 25% validation (del 80% anterior)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.25, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(f\"üìä Divisi√≥n de datos:\")\n",
    "print(f\"   Train:      {len(X_train):,} muestras ({len(X_train)/len(X)*100:.1f}%)\")\n",
    "print(f\"   Validation: {len(X_val):,} muestras ({len(X_val)/len(X)*100:.1f}%)\")\n",
    "print(f\"   Test:       {len(X_test):,} muestras ({len(X_test)/len(X)*100:.1f}%)\")\n",
    "print(f\"\\n   Total: {len(X):,} muestras\")\n",
    "print(f\"   Features: {X.shape[1]}\")\n",
    "print(f\"   Clases: {num_classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "normalization",
   "metadata": {},
   "source": [
    "## 10. Normalizaci√≥n de Features\n",
    "\n",
    "Importante para redes neuronales: todas las features deben estar en la misma escala."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scale_features",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìè Normalizando features con StandardScaler...\\n\")\n",
    "\n",
    "# Crear escalador\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Ajustar solo con datos de entrenamiento\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convertir a DataFrames para mantener nombres de columnas\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)\n",
    "X_val_scaled = pd.DataFrame(X_val_scaled, columns=X_val.columns, index=X_val.index)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)\n",
    "\n",
    "print(\"‚úÖ Normalizaci√≥n completada\")\n",
    "print(f\"\\nüìä Estad√≠sticas del conjunto de entrenamiento normalizado:\")\n",
    "print(X_train_scaled.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "save_data",
   "metadata": {},
   "source": [
    "## 11. Guardar Datos Procesados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save_processed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "# Crear directorio para datos procesados\n",
    "os.makedirs('processed_data', exist_ok=True)\n",
    "\n",
    "print(\"üíæ Guardando datos procesados...\\n\")\n",
    "\n",
    "# Guardar conjuntos de datos\n",
    "np.save('processed_data/X_train.npy', X_train_scaled.values)\n",
    "np.save('processed_data/X_val.npy', X_val_scaled.values)\n",
    "np.save('processed_data/X_test.npy', X_test_scaled.values)\n",
    "np.save('processed_data/y_train.npy', y_train.values)\n",
    "np.save('processed_data/y_val.npy', y_val.values)\n",
    "np.save('processed_data/y_test.npy', y_test.values)\n",
    "\n",
    "# Guardar scaler y encoders\n",
    "with open('processed_data/scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "with open('processed_data/label_encoders.pkl', 'wb') as f:\n",
    "    pickle.dump(label_encoders, f)\n",
    "\n",
    "with open('processed_data/target_encoder.pkl', 'wb') as f:\n",
    "    pickle.dump(target_encoder, f)\n",
    "\n",
    "# Guardar nombres de features\n",
    "with open('processed_data/feature_names.pkl', 'wb') as f:\n",
    "    pickle.dump(all_features, f)\n",
    "\n",
    "# Guardar metadata\n",
    "metadata = {\n",
    "    'num_classes': num_classes,\n",
    "    'num_features': len(all_features),\n",
    "    'train_samples': len(X_train),\n",
    "    'val_samples': len(X_val),\n",
    "    'test_samples': len(X_test),\n",
    "    'feature_names': all_features,\n",
    "    'class_names': target_encoder.classes_.tolist()\n",
    "}\n",
    "\n",
    "with open('processed_data/metadata.pkl', 'wb') as f:\n",
    "    pickle.dump(metadata, f)\n",
    "\n",
    "print(\"‚úÖ Datos guardados en 'processed_data/':\")\n",
    "print(\"   - X_train.npy, X_val.npy, X_test.npy\")\n",
    "print(\"   - y_train.npy, y_val.npy, y_test.npy\")\n",
    "print(\"   - scaler.pkl\")\n",
    "print(\"   - label_encoders.pkl\")\n",
    "print(\"   - target_encoder.pkl\")\n",
    "print(\"   - feature_names.pkl\")\n",
    "print(\"   - metadata.pkl\")\n",
    "\n",
    "print(f\"\\nüì¶ Tama√±o total de archivos procesados:\")\n",
    "total_size = sum(os.path.getsize(f'processed_data/{f}') \n",
    "                 for f in os.listdir('processed_data/')) / (1024**2)\n",
    "print(f\"   {total_size:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "time_series_prep",
   "metadata": {},
   "source": [
    "## 12. Preparaci√≥n de Datos para Series Temporales (LSTM/GRU)\n",
    "\n",
    "Para modelos recurrentes, necesitamos organizar los datos como secuencias temporales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prepare_timeseries",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìà Preparando datos para series temporales...\\n\")\n",
    "\n",
    "# Agregar datos por d√≠a y tipo de delito\n",
    "df_timeseries = df.copy()\n",
    "df_timeseries['fecha'] = df_timeseries['fecha_hecho'].dt.date\n",
    "\n",
    "# Contar delitos por d√≠a y categor√≠a\n",
    "delitos_por_dia = df_timeseries.groupby(['fecha', 'categoria_delito']).size().reset_index(name='count')\n",
    "\n",
    "# Pivotear para tener una columna por categor√≠a de delito\n",
    "delitos_pivot = delitos_por_dia.pivot(index='fecha', columns='categoria_delito', values='count').fillna(0)\n",
    "\n",
    "# Agregar total de delitos por d√≠a\n",
    "delitos_pivot['total_delitos'] = delitos_pivot.sum(axis=1)\n",
    "\n",
    "# Resetear √≠ndice\n",
    "delitos_pivot = delitos_pivot.reset_index()\n",
    "delitos_pivot['fecha'] = pd.to_datetime(delitos_pivot['fecha'])\n",
    "delitos_pivot = delitos_pivot.sort_values('fecha')\n",
    "\n",
    "print(f\"‚úÖ Serie temporal creada:\")\n",
    "print(f\"   Fechas: {delitos_pivot['fecha'].min()} a {delitos_pivot['fecha'].max()}\")\n",
    "print(f\"   D√≠as: {len(delitos_pivot)}\")\n",
    "print(f\"   Categor√≠as de delitos: {len(delitos_pivot.columns) - 2}\")\n",
    "\n",
    "# Guardar datos de series temporales\n",
    "delitos_pivot.to_csv('processed_data/timeseries_data.csv', index=False)\n",
    "print(\"\\nüíæ Guardado en 'processed_data/timeseries_data.csv'\")\n",
    "\n",
    "# Visualizaci√≥n r√°pida\n",
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "ax.plot(delitos_pivot['fecha'], delitos_pivot['total_delitos'], linewidth=1)\n",
    "ax.set_xlabel('Fecha')\n",
    "ax.set_ylabel('Total de Delitos por D√≠a')\n",
    "ax.set_title('Serie Temporal de Delitos en CDMX')\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spatial_grid_prep",
   "metadata": {},
   "source": [
    "## 13. Preparaci√≥n de Grid Espacial para CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create_spatial_grid",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üó∫Ô∏è  Creando grid espacial para CNN...\\n\")\n",
    "\n",
    "# Crear matriz 2D de densidad delictiva\n",
    "grid_size = 50\n",
    "\n",
    "# Crear histograma 2D\n",
    "heatmap, xedges, yedges = np.histogram2d(\n",
    "    df['latitud'], \n",
    "    df['longitud'],\n",
    "    bins=grid_size\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Grid espacial creado: {heatmap.shape}\")\n",
    "print(f\"   Rango latitud: {xedges[0]:.4f} a {xedges[-1]:.4f}\")\n",
    "print(f\"   Rango longitud: {yedges[0]:.4f} a {yedges[-1]:.4f}\")\n",
    "print(f\"   Total delitos en grid: {heatmap.sum():.0f}\")\n",
    "\n",
    "# Guardar grid\n",
    "np.save('processed_data/spatial_grid.npy', heatmap)\n",
    "np.save('processed_data/spatial_edges.npy', {'lat': xedges, 'lon': yedges})\n",
    "\n",
    "# Visualizaci√≥n\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "im = ax.imshow(heatmap.T, origin='lower', cmap='hot', interpolation='bilinear')\n",
    "ax.set_title('Mapa de Calor de Delitos en CDMX (Grid 50x50)', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Latitud (bins)')\n",
    "ax.set_ylabel('Longitud (bins)')\n",
    "plt.colorbar(im, ax=ax, label='N√∫mero de Delitos')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüíæ Guardado en 'processed_data/spatial_grid.npy'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## 14. Resumen del Preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final_summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\" \"*25 + \"RESUMEN DEL PREPROCESAMIENTO\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nüìä DATOS ORIGINALES:\")\n",
    "print(f\"   Registros totales: {total_rows:,}\")\n",
    "print(f\"   Muestra utilizada: {len(df):,} ({sample_fraction*100}%)\")\n",
    "print(f\"   Columnas originales: {len(df_spark.columns)}\")\n",
    "\n",
    "print(f\"\\nüîß FEATURES CREADAS:\")\n",
    "print(f\"   Features temporales: {len([f for f in all_features if any(t in f for t in ['a√±o', 'mes', 'dia', 'hora', 'semana', 'trimestre', 'sin', 'cos'])])}\")\n",
    "print(f\"   Features geogr√°ficas: {len([f for f in all_features if any(g in f for g in ['latitud', 'longitud', 'densidad'])])}\")\n",
    "print(f\"   Features categ√≥ricas: {len(categorical_encoded)}\")\n",
    "print(f\"   Total features: {len(all_features)}\")\n",
    "\n",
    "print(f\"\\n‚úÇÔ∏è DIVISI√ìN DE DATOS:\")\n",
    "print(f\"   Entrenamiento: {len(X_train):,} ({len(X_train)/len(X)*100:.1f}%)\")\n",
    "print(f\"   Validaci√≥n: {len(X_val):,} ({len(X_val)/len(X)*100:.1f}%)\")\n",
    "print(f\"   Prueba: {len(X_test):,} ({len(X_test)/len(X)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nüéØ VARIABLE OBJETIVO:\")\n",
    "print(f\"   N√∫mero de clases: {num_classes}\")\n",
    "print(f\"   Clase m√°s frecuente: {df['delito'].value_counts().index[0]}\")\n",
    "print(f\"   Frecuencia: {df['delito'].value_counts().iloc[0]:,}\")\n",
    "\n",
    "print(f\"\\nüíæ ARCHIVOS GUARDADOS:\")\n",
    "print(f\"   ‚úì Conjuntos train/val/test (NumPy)\")\n",
    "print(f\"   ‚úì Scaler y encoders (pickle)\")\n",
    "print(f\"   ‚úì Metadata del proyecto\")\n",
    "print(f\"   ‚úì Datos de series temporales (CSV)\")\n",
    "print(f\"   ‚úì Grid espacial para CNN (NumPy)\")\n",
    "\n",
    "print(f\"\\n‚úÖ PREPROCESAMIENTO COMPLETADO\")\n",
    "print(f\"   Los datos est√°n listos para entrenar redes neuronales\")\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "next_steps",
   "metadata": {},
   "source": [
    "## 15. Pr√≥ximos Pasos\n",
    "\n",
    "Con los datos preprocesados y guardados, podemos proceder a:\n",
    "\n",
    "1. **MLP Classification** (`02_MLP_Classification.ipynb`): Red neuronal feedforward para clasificaci√≥n multi-clase\n",
    "2. **LSTM Time Series** (`03_LSTM_TimeSeries.ipynb`): Predicci√≥n de tendencias temporales\n",
    "3. **GRU Time Series** (`04_GRU_TimeSeries.ipynb`): Comparaci√≥n con LSTM\n",
    "4. **CNN Spatial** (`05_CNN_Spatial.ipynb`): An√°lisis de patrones espaciales\n",
    "5. **Autoencoder** (`06_Autoencoder_Anomalies.ipynb`): Detecci√≥n de anomal√≠as\n",
    "\n",
    "---\n",
    "\n",
    "**Autor**: Adonnay Bazaldua  \n",
    "**Proyecto**: An√°lisis de Delitos CDMX con Redes Neuronales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleanup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detener Spark Session\n",
    "spark.stop()\n",
    "print(\"üõë Spark Session detenida\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
