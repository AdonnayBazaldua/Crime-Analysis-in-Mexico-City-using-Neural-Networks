{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "apz4nx-59Zs9",
        "outputId": "7a1bea71-0aaf-4d33-ce7f-ac6c4f71f142"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
            "â•‘                                          â•‘\n",
            "â•‘            ANALYTICS Y MACHINE LEARNING CON SPARKML                 â•‘\n",
            "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
            "\n",
            "En esta semana aprenderÃ¡s a:\n",
            "\n",
            " âœ“ Usar SparkML Pipeline API para ML reproducible\n",
            " âœ“ Transformar features con VectorAssembler y otros transformers\n",
            " âœ“ Entrenar clasificador masivo (ej. RegresiÃ³n LogÃ­stica)\n",
            " âœ“ Evaluar modelos (accuracy, ROC-AUC, F1)\n",
            " âœ“ Optimizar hiperparÃ¡metros de forma distribuida\n",
            "\n",
            "ðŸ’¡ SparkML estÃ¡ diseÃ±ado para trabajar con datasets masivos y DataFrames.\n",
            "ðŸŽ¯ Caso prÃ¡ctico: Â¿Viaje tendrÃ¡ propina alta?\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# ANALYTICS Y MACHINE LEARNING CON SPARKML\n",
        "# ============================================================================\n",
        "# Objetivos de aprendizaje:\n",
        "# 1. Comprender arquitectura de SparkML y Pipelines\n",
        "# 2. Transformar datos para ML\n",
        "# 3. Construir y evaluar modelos de clasificaciÃ³n en Spark\n",
        "# 4. Comparar SparkML con scikit-learn en escalabilidad\n",
        "# Dataset: NYC Taxi (propina alta sÃ­/no)\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\"\"\n",
        "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
        "â•‘                                          â•‘\n",
        "â•‘            ANALYTICS Y MACHINE LEARNING CON SPARKML                 â•‘\n",
        "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "En esta semana aprenderÃ¡s a:\n",
        "\n",
        " âœ“ Usar SparkML Pipeline API para ML reproducible\n",
        " âœ“ Transformar features con VectorAssembler y otros transformers\n",
        " âœ“ Entrenar clasificador masivo (ej. RegresiÃ³n LogÃ­stica)\n",
        " âœ“ Evaluar modelos (accuracy, ROC-AUC, F1)\n",
        " âœ“ Optimizar hiperparÃ¡metros de forma distribuida\n",
        "\n",
        "ðŸ’¡ SparkML estÃ¡ diseÃ±ado para trabajar con datasets masivos y DataFrames.\n",
        "ðŸŽ¯ Caso prÃ¡ctico: Â¿Viaje tendrÃ¡ propina alta?\n",
        "\"\"\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# InstalaciÃ³n rÃ¡pida\n",
        "!apt-get update -qq\n",
        "!apt-get install -y openjdk-11-jdk-headless -qq\n",
        "!pip install -q pyspark findspark\n",
        "\n",
        "# Configurar\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "\n",
        "# Inicializar\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .appName(\"MiApp\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "print(\"âœ… Spark listo!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V67fq3ct_X_n",
        "outputId": "ad3b20d4-ec27-44da-b6c7-b7e7da190d17"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Selecting previously unselected package openjdk-11-jre-headless:amd64.\n",
            "(Reading database ... 121713 files and directories currently installed.)\n",
            "Preparing to unpack .../openjdk-11-jre-headless_11.0.29+7-1ubuntu1~22.04_amd64.deb ...\n",
            "Unpacking openjdk-11-jre-headless:amd64 (11.0.29+7-1ubuntu1~22.04) ...\n",
            "Selecting previously unselected package openjdk-11-jdk-headless:amd64.\n",
            "Preparing to unpack .../openjdk-11-jdk-headless_11.0.29+7-1ubuntu1~22.04_amd64.deb ...\n",
            "Unpacking openjdk-11-jdk-headless:amd64 (11.0.29+7-1ubuntu1~22.04) ...\n",
            "Setting up openjdk-11-jre-headless:amd64 (11.0.29+7-1ubuntu1~22.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jjs to provide /usr/bin/jjs (jjs) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/rmid to provide /usr/bin/rmid (rmid) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/pack200 to provide /usr/bin/pack200 (pack200) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/unpack200 to provide /usr/bin/unpack200 (unpack200) in auto mode\n",
            "Setting up openjdk-11-jdk-headless:amd64 (11.0.29+7-1ubuntu1~22.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/rmic to provide /usr/bin/rmic (rmic) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jaotc to provide /usr/bin/jaotc (jaotc) in auto mode\n",
            "âœ… Spark listo!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, when, hour, dayofweek, avg, count, round as spark_round\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, TimestampType, DoubleType, StringType\n",
        "\n",
        "# Importar SparkML\n",
        "from pyspark.ml.feature import VectorAssembler, StandardScaler, StringIndexer, OneHotEncoder\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator"
      ],
      "metadata": {
        "id": "kIDdnRiY927u"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# GENERACIÃ“N DEL DATASET NYC TAXI PARA MACHINE LEARNING\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ðŸ“‚ GENERACIÃ“N DE DATOS PARA MACHINE LEARNING\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\"\"\n",
        "ðŸ’¡ Importante:\n",
        "  Usaremos datos sintÃ©ticos que simulan viajes de taxi.\n",
        "  - Objetivo (target): high_tip (1 si tip/fare > 20%)\n",
        "  - Features: trip_distance, passenger_count, hour_of_day, day_of_week, trip_duration\n",
        "\"\"\")\n",
        "\n",
        "def generar_datos_taxi_ml(n_registros=15000):\n",
        "    import random\n",
        "    from datetime import datetime, timedelta\n",
        "    random.seed(42)\n",
        "    base_date = datetime(2024, 1, 1)\n",
        "    data = []\n",
        "    for i in range(n_registros):\n",
        "        trip_id = i + 1\n",
        "        pickup_dt = base_date + timedelta(\n",
        "            days=random.randint(0, 30),\n",
        "            hours=random.randint(0, 23),\n",
        "            minutes=random.randint(0, 59)\n",
        "        )\n",
        "        trip_duration_minutes = random.randint(5, 90)\n",
        "        dropoff_dt = pickup_dt + timedelta(minutes=trip_duration_minutes)\n",
        "        passenger_count = random.randint(1, 6)\n",
        "        trip_distance = round(random.uniform(0.5, 25.0), 2)\n",
        "        fare_amount = round(2.5 + (trip_distance * 2.5) + random.uniform(-3, 5), 2)\n",
        "        fare_amount = max(fare_amount, 3.0)  # MÃ­nimo $3\n",
        "        hour_of_day = pickup_dt.hour\n",
        "        tip_factor = 0.15\n",
        "        if hour_of_day in [7, 8, 17, 18, 19, 20]: tip_factor += 0.08\n",
        "        if trip_distance > 10: tip_factor += 0.05\n",
        "        if passenger_count >= 3: tip_factor += 0.04\n",
        "        tip_factor += random.uniform(-0.05, 0.10)\n",
        "        tip_factor = max(0, min(tip_factor, 0.40))\n",
        "        tip_amount = round(fare_amount * tip_factor, 2)\n",
        "        high_tip = 1 if fare_amount > 0 and (tip_amount/fare_amount) > 0.2 else 0\n",
        "        data.append((trip_id, pickup_dt, dropoff_dt, passenger_count, trip_distance, fare_amount, tip_amount, high_tip, trip_duration_minutes))\n",
        "    return data\n",
        "\n",
        "schema = StructType([\n",
        "    StructField(\"trip_id\", IntegerType(), True),\n",
        "    StructField(\"tpep_pickup_datetime\", TimestampType(), True),\n",
        "    StructField(\"tpep_dropoff_datetime\", TimestampType(), True),\n",
        "    StructField(\"passenger_count\", IntegerType(), True),\n",
        "    StructField(\"trip_distance\", DoubleType(), True),\n",
        "    StructField(\"fare_amount\", DoubleType(), True),\n",
        "    StructField(\"tip_amount\", DoubleType(), True),\n",
        "    StructField(\"high_tip\", IntegerType(), True),\n",
        "    StructField(\"trip_duration\", IntegerType(), True),\n",
        "])\n",
        "\n",
        "print(\"â³ Generando 15,000 registros de viajes de taxi...\")\n",
        "datos_taxi = generar_datos_taxi_ml(15000)\n",
        "df_taxi = spark.createDataFrame(datos_taxi, schema=schema)\n",
        "print(f\"âœ… Datos generados. Total de registros: {df_taxi.count()}\")\n",
        "df_taxi.show(10, truncate=False)\n",
        "print(\"\\nðŸŽ¯ DistribuciÃ³n de la variable objetivo (high_tip):\")\n",
        "df_taxi.groupBy(\"high_tip\").count().show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9X935cQ916Q",
        "outputId": "802ad2de-e405-40c1-fbfa-b329a78fd061"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "ðŸ“‚ GENERACIÃ“N DE DATOS PARA MACHINE LEARNING\n",
            "======================================================================\n",
            "\n",
            "ðŸ’¡ Importante:\n",
            "  Usaremos datos sintÃ©ticos que simulan viajes de taxi.\n",
            "  - Objetivo (target): high_tip (1 si tip/fare > 20%)\n",
            "  - Features: trip_distance, passenger_count, hour_of_day, day_of_week, trip_duration\n",
            "\n",
            "â³ Generando 15,000 registros de viajes de taxi...\n",
            "âœ… Datos generados. Total de registros: 15000\n",
            "+-------+--------------------+---------------------+---------------+-------------+-----------+----------+--------+-------------+\n",
            "|trip_id|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|fare_amount|tip_amount|high_tip|trip_duration|\n",
            "+-------+--------------------+---------------------+---------------+-------------+-----------+----------+--------+-------------+\n",
            "|1      |2024-01-21 03:01:00 |2024-01-21 03:41:00  |2              |5.97         |20.32      |4.09      |1       |40           |\n",
            "|2      |2024-01-29 17:05:00 |2024-01-29 18:25:00  |4              |1.28         |3.45       |0.88      |1       |80           |\n",
            "|3      |2024-01-20 00:35:00 |2024-01-20 01:05:00  |6              |16.42        |44.91      |10.02     |1       |30           |\n",
            "|4      |2024-01-19 08:51:00 |2024-01-19 08:56:00  |2              |17.6         |46.22      |11.71     |1       |5            |\n",
            "|5      |2024-01-31 10:06:00 |2024-01-31 10:22:00  |4              |2.87         |13.45      |3.1       |1       |16           |\n",
            "|6      |2024-01-26 01:46:00 |2024-01-26 02:49:00  |5              |3.56         |15.78      |2.4       |0       |63           |\n",
            "|7      |2024-01-10 20:39:00 |2024-01-10 21:30:00  |5              |5.21         |13.08      |4.17      |1       |51           |\n",
            "|8      |2024-01-25 09:05:00 |2024-01-25 09:39:00  |1              |9.81         |27.65      |6.22      |1       |34           |\n",
            "|9      |2024-01-06 11:22:00 |2024-01-06 11:53:00  |6              |7.04         |24.59      |5.83      |1       |31           |\n",
            "|10     |2024-01-20 20:10:00 |2024-01-20 21:23:00  |6              |6.5          |19.45      |5.07      |1       |73           |\n",
            "+-------+--------------------+---------------------+---------------+-------------+-----------+----------+--------+-------------+\n",
            "only showing top 10 rows\n",
            "\n",
            "\n",
            "ðŸŽ¯ DistribuciÃ³n de la variable objetivo (high_tip):\n",
            "+--------+-----+\n",
            "|high_tip|count|\n",
            "+--------+-----+\n",
            "|       1|11682|\n",
            "|       0| 3318|\n",
            "+--------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# PREPARACIÃ“N DE FEATURES PARA MACHINE LEARNING\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"ðŸ”¬ TRANSFORMACIÃ“N DE VARIABLES PARA ML CON SPARK\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\"\"\n",
        "Las variables seleccionadas para el modelo serÃ¡n:\n",
        "    - trip_distance\n",
        "    - passenger_count\n",
        "    - trip_duration\n",
        "    - hour_of_day (extraÃ­da de tpep_pickup_datetime)\n",
        "    - day_of_week (extraÃ­da de tpep_pickup_datetime)\n",
        "\"\"\")\n",
        "\n",
        "# Extraer variables temporales\n",
        "from pyspark.sql.functions import hour, dayofweek\n",
        "\n",
        "df_ml = df_taxi.withColumn(\"hour_of_day\", hour(col(\"tpep_pickup_datetime\"))) \\\n",
        "    .withColumn(\"day_of_week\", dayofweek(col(\"tpep_pickup_datetime\")))\n",
        "\n",
        "features = [\n",
        "    \"trip_distance\",\n",
        "    \"passenger_count\",\n",
        "    \"trip_duration\",\n",
        "    \"hour_of_day\",\n",
        "    \"day_of_week\"\n",
        "]\n",
        "\n",
        "target = \"high_tip\"\n",
        "\n",
        "print(f\"âœ… Variables para VectorAssembler: {features}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dt-rETTf_rI-",
        "outputId": "bee61a1a-a6c5-4fda-d349-6d5f1fd71a31"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "ðŸ”¬ TRANSFORMACIÃ“N DE VARIABLES PARA ML CON SPARK\n",
            "======================================================================\n",
            "\n",
            "Las variables seleccionadas para el modelo serÃ¡n:\n",
            "    - trip_distance\n",
            "    - passenger_count\n",
            "    - trip_duration\n",
            "    - hour_of_day (extraÃ­da de tpep_pickup_datetime)\n",
            "    - day_of_week (extraÃ­da de tpep_pickup_datetime)\n",
            "\n",
            "âœ… Variables para VectorAssembler: ['trip_distance', 'passenger_count', 'trip_duration', 'hour_of_day', 'day_of_week']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# PIPELINE DE MACHINE LEARNING CON SPARKML\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"âš¡ PIPELINE: PREPARAR, ENTRENAR Y EVALUAR UN MODELO EN SPARKML\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
        "\n",
        "# Paso 1: Ensamblar variables numÃ©ricas\n",
        "assembler = VectorAssembler(inputCols=features, outputCol=\"features_vector\")\n",
        "\n",
        "# Paso 2: Escalado de features\n",
        "scaler = StandardScaler(inputCol=\"features_vector\", outputCol=\"features\", withMean=True, withStd=True)\n",
        "\n",
        "# Paso 3: Modelo de clasificaciÃ³n: RegresiÃ³n LogÃ­stica\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "\n",
        "lr = LogisticRegression(featuresCol=\"features\", labelCol=target)\n",
        "\n",
        "# Construir pipeline\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "pipeline = Pipeline(stages=[assembler, scaler, lr])\n",
        "\n",
        "print(\"âœ… Pipeline construido: [VectorAssembler â†’ StandardScaler â†’ LogisticRegression]\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GaOvFrBu_4Pi",
        "outputId": "3951f581-5a54-4364-96f6-282bf49abeac"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "âš¡ PIPELINE: PREPARAR, ENTRENAR Y EVALUAR UN MODELO EN SPARKML\n",
            "======================================================================\n",
            "âœ… Pipeline construido: [VectorAssembler â†’ StandardScaler â†’ LogisticRegression]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# ENTRENAMIENTO Y EVALUACIÃ“N DE UN MODELO DE CLASIFICACIÃ“N BINARIA\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"ðŸš€ ENTRENANDO Y EVALUANDO REGRESIÃ“N LOGÃSTICA EN SPARKML\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Dividir en training/test\n",
        "train_df, test_df = df_ml.randomSplit([0.7, 0.3], seed=42)\n",
        "print(f\"Registros para entrenamiento: {train_df.count()}, para testing: {test_df.count()}\")\n",
        "\n",
        "# Ajustar pipeline al set de entrenamiento\n",
        "modelo = pipeline.fit(train_df)\n",
        "\n",
        "# PredicciÃ³n sobre test\n",
        "predicciones = modelo.transform(test_df)\n",
        "\n",
        "print(\"\\nðŸŽ¯ Ejemplo de predicciones:\")\n",
        "predicciones.select(\"features\", \"high_tip\", \"prediction\", \"probability\").show(5)\n",
        "\n",
        "# EvaluaciÃ³n\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "\n",
        "evaluator = BinaryClassificationEvaluator(labelCol=target, rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")\n",
        "roc_auc = evaluator.evaluate(predicciones)\n",
        "\n",
        "accuracy = predicciones.filter(col(\"high_tip\") == col(\"prediction\")).count() / test_df.count() * 100\n",
        "\n",
        "print(f\"\\nâœ… ROC-AUC: {roc_auc:.2f}\")\n",
        "print(f\"âœ… ACCURACY: {accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_P8ThH2XBwnU",
        "outputId": "6d834516-2939-40da-b487-787a37a3d7b8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "ðŸš€ ENTRENANDO Y EVALUANDO REGRESIÃ“N LOGÃSTICA EN SPARKML\n",
            "======================================================================\n",
            "Registros para entrenamiento: 10649, para testing: 4351\n",
            "\n",
            "ðŸŽ¯ Ejemplo de predicciones:\n",
            "+--------------------+--------+----------+--------------------+\n",
            "|            features|high_tip|prediction|         probability|\n",
            "+--------------------+--------+----------+--------------------+\n",
            "|[0.52991045230484...|       1|       1.0|[0.09008428361714...|\n",
            "|[-1.0604821066281...|       1|       1.0|[0.19388431991966...|\n",
            "|[-0.8008551322438...|       1|       1.0|[0.16049387485182...|\n",
            "|[-0.8774663705867...|       1|       1.0|[0.12666765411161...|\n",
            "|[0.22204677229730...|       1|       1.0|[0.08915727533591...|\n",
            "+--------------------+--------+----------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "\n",
            "âœ… ROC-AUC: 0.56\n",
            "âœ… ACCURACY: 78.60%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# UPDATE:  Usar one-hot encoding para las variables hour_of_day y day_of_week\n",
        "# ============================================================================\n",
        "\n",
        "# Extraer variables temporales\n",
        "from pyspark.sql.functions import hour, dayofweek, col\n",
        "df_ml = df_taxi.withColumn(\"hour_of_day\", hour(col(\"tpep_pickup_datetime\"))) \\\n",
        "    .withColumn(\"day_of_week\", dayofweek(col(\"tpep_pickup_datetime\")))\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"âš¡ PIPELINE: PREPARAR, ENTRENAR Y EVALUAR UN MODELO EN SPARKML\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "from pyspark.ml.feature import VectorAssembler, StandardScaler, OneHotEncoder, StringIndexer\n",
        "\n",
        "# Paso 1: Convertir variables temporales a Ã­ndices y luego a one-hot encoding\n",
        "hour_indexer = StringIndexer(inputCol=\"hour_of_day\", outputCol=\"hour_indexed\")\n",
        "hour_encoder = OneHotEncoder(inputCol=\"hour_indexed\", outputCol=\"hour_encoded\")\n",
        "\n",
        "day_indexer = StringIndexer(inputCol=\"day_of_week\", outputCol=\"day_indexed\")\n",
        "day_encoder = OneHotEncoder(inputCol=\"day_indexed\", outputCol=\"day_encoded\")\n",
        "\n",
        "# Paso 2a: Ensamblar SOLO las variables numÃ©ricas para escalar\n",
        "numerical_features = [\n",
        "    \"trip_distance\",\n",
        "    \"passenger_count\",\n",
        "    \"trip_duration\"\n",
        "]\n",
        "\n",
        "numerical_assembler = VectorAssembler(inputCols=numerical_features, outputCol=\"numerical_features\")\n",
        "\n",
        "# Paso 2b: Escalar SOLO las variables numÃ©ricas\n",
        "scaler = StandardScaler(inputCol=\"numerical_features\", outputCol=\"numerical_features_scaled\",\n",
        "                        withMean=True, withStd=True)\n",
        "\n",
        "# Paso 3: Ensamblar TODO (numÃ©ricas escaladas + one-hot sin escalar)\n",
        "final_features = [\n",
        "    \"numerical_features_scaled\",\n",
        "    \"hour_encoded\",\n",
        "    \"day_encoded\"\n",
        "]\n",
        "\n",
        "final_assembler = VectorAssembler(inputCols=final_features, outputCol=\"features\")\n",
        "\n",
        "print(f\"âœ… Variables numÃ©ricas a escalar: {numerical_features}\")\n",
        "print(f\"âœ… Features finales ensambladas: {final_features}\")\n",
        "\n",
        "# Paso 4: Modelo de clasificaciÃ³n: RegresiÃ³n LogÃ­stica\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "target = \"high_tip\"\n",
        "lr = LogisticRegression(featuresCol=\"features\", labelCol=target)\n",
        "\n",
        "# Construir pipeline completo\n",
        "from pyspark.ml import Pipeline\n",
        "pipeline = Pipeline(stages=[\n",
        "    hour_indexer, hour_encoder,\n",
        "    day_indexer, day_encoder,\n",
        "    numerical_assembler, scaler,\n",
        "    final_assembler,\n",
        "    lr\n",
        "])\n",
        "\n",
        "print(\"âœ… Pipeline construido: [StringIndexer â†’ OneHotEncoder â†’ VectorAssembler(numÃ©ricos) â†’ StandardScaler â†’ VectorAssembler(final) â†’ LogisticRegression]\")\n",
        "print(\"=\"*70)\n",
        "print(\"ðŸš€ ENTRENANDO Y EVALUANDO REGRESIÃ“N LOGÃSTICA EN SPARKML\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Dividir en training/test\n",
        "train_df, test_df = df_ml.randomSplit([0.7, 0.3], seed=42)\n",
        "print(f\"Registros para entrenamiento: {train_df.count()}, para testing: {test_df.count()}\")\n",
        "\n",
        "# Ajustar pipeline al set de entrenamiento\n",
        "modelo = pipeline.fit(train_df)\n",
        "\n",
        "# PredicciÃ³n sobre test\n",
        "predicciones = modelo.transform(test_df)\n",
        "\n",
        "print(\"\\nðŸŽ¯ Ejemplo de predicciones:\")\n",
        "predicciones.select(\"features\", \"high_tip\", \"prediction\", \"probability\").show(5)\n",
        "\n",
        "# EvaluaciÃ³n\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "evaluator = BinaryClassificationEvaluator(labelCol=target, rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")\n",
        "roc_auc = evaluator.evaluate(predicciones)\n",
        "\n",
        "accuracy = predicciones.filter(col(\"high_tip\") == col(\"prediction\")).count() / test_df.count() * 100\n",
        "\n",
        "print(f\"\\nðŸ“Š RESULTADOS DEL MODELO:\")\n",
        "print(f\"   - ROC-AUC: {roc_auc:.4f}\")\n",
        "print(f\"   - Accuracy: {accuracy:.2f}%\")\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MjBjRnj1sEfC",
        "outputId": "1dbc2f01-fe26-4026-de4b-ca445b5568d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "âš¡ PIPELINE: PREPARAR, ENTRENAR Y EVALUAR UN MODELO EN SPARKML\n",
            "======================================================================\n",
            "âœ… Variables numÃ©ricas a escalar: ['trip_distance', 'passenger_count', 'trip_duration']\n",
            "âœ… Features finales ensambladas: ['numerical_features_scaled', 'hour_encoded', 'day_encoded']\n",
            "âœ… Pipeline construido: [StringIndexer â†’ OneHotEncoder â†’ VectorAssembler(numÃ©ricos) â†’ StandardScaler â†’ VectorAssembler(final) â†’ LogisticRegression]\n",
            "======================================================================\n",
            "ðŸš€ ENTRENANDO Y EVALUANDO REGRESIÃ“N LOGÃSTICA EN SPARKML\n",
            "======================================================================\n",
            "Registros para entrenamiento: 10649, para testing: 4351\n",
            "\n",
            "ðŸŽ¯ Ejemplo de predicciones:\n",
            "+--------------------+--------+----------+--------------------+\n",
            "|            features|high_tip|prediction|         probability|\n",
            "+--------------------+--------+----------+--------------------+\n",
            "|(32,[0,1,2,25],[0...|       1|       1.0|[0.08898505517088...|\n",
            "|(32,[0,1,2,5,26],...|       1|       1.0|[0.01388994549747...|\n",
            "|(32,[0,1,2,8],[-0...|       1|       1.0|[0.24674475571671...|\n",
            "|(32,[0,1,2,5],[-0...|       1|       1.0|[0.01031594027385...|\n",
            "|(32,[0,1,2,3,31],...|       1|       1.0|[0.00281354893546...|\n",
            "+--------------------+--------+----------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "\n",
            "ðŸ“Š RESULTADOS DEL MODELO:\n",
            "   - ROC-AUC: 0.6450\n",
            "   - Accuracy: 80.53%\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tarea\n",
        "\n",
        "#1. Modifica el pipeline para usar RandomForestClassifier y compara mÃ©tricas.\n",
        "#2. Cambia la variable objetivo a propina mayor a 10% (tip_amount/fare_amount > 0.1) y repite el proceso."
      ],
      "metadata": {
        "id": "i9WTvXQgHeG0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}